{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28621a1e",
   "metadata": {},
   "source": [
    "# Example of the usage of the Weak label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4dcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29cb012",
   "metadata": {},
   "source": [
    "We first need to load:\n",
    "\n",
    "1. **Standard Python libraries** for data handling and reproducibility.  \n",
    "2. **PyTorch** (and its submodules) for model definition, training, and data loading.  \n",
    "3. **Custom modules** from this project:\n",
    "   - **`train_test_loop`**: provides the `train_and_evaluate` function to run training and evaluation loops.  \n",
    "   - **`losses`**: contains various weak‐label‐aware loss functions like `FwdBwdLoss`.  \n",
    "   - **`weakener`**: implements the `Weakener` class for generating noisy/weak labels.  \n",
    "   - **`model`**: defines model architectures .\n",
    "   - **`dataset`**: provides `Data_handling` (and other dataset classes) for loading and splitting data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62fdf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# PyTorch core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom project modules\n",
    "from utils.train_test_loop import train_and_evaluate\n",
    "from utils.losses import FwdLoss, EMLoss, FwdBwdLoss, MarginalChainLoss\n",
    "from utils.losses1 import MarginalChainProperLoss, ForwardProperLoss, scoring_matrix\n",
    "from utils.losses1 import UpperBoundWeakProperLoss\n",
    "from utils.dataset_visualization import visualize_dataset\n",
    "from src.weakener import Weakener\n",
    "from src.model import MLP\n",
    "from src.dataset import Data_handling\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db405b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73f12c25",
   "metadata": {},
   "source": [
    "## Loading and Visualizing Iris\n",
    "\n",
    "1. **Instantiate** our `Data_handling` class to load the Iris dataset from OpenML (ID 61) using an 80/20 train/test split.  \n",
    "2. **Retrieve** the raw arrays of features and labels via `get_data()`.  \n",
    "3. **Combine** the train and test portions back into a single DataFrame \n",
    "4. **Visualize** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb6a1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e44cfc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dataset_name = \u001b[33m'\u001b[39m\u001b[33mCifar10\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m Data = \u001b[43mData_handling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset='mnist',\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplitting_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC-1/src/dataset.py:171\u001b[39m, in \u001b[36mData_handling.__init__\u001b[39m\u001b[34m(self, dataset, train_size, test_size, valid_size, batch_size, shuffling, splitting_seed)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28mself\u001b[39m.transform = transforms.Compose([\n\u001b[32m    161\u001b[39m     transforms.RandomCrop(\u001b[32m32\u001b[39m, padding=\u001b[32m4\u001b[39m),\n\u001b[32m    162\u001b[39m     transforms.RandomHorizontalFlip(),\n\u001b[32m    163\u001b[39m     transforms.ToTensor(),\n\u001b[32m    164\u001b[39m     transforms.Normalize((\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m), (\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m))\n\u001b[32m    165\u001b[39m     ])\n\u001b[32m    166\u001b[39m \u001b[38;5;28mself\u001b[39m.train_dataset = datasets.\u001b[34m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m.dataset](\n\u001b[32m    167\u001b[39m     root=\u001b[33m'\u001b[39m\u001b[33mDatasets/raw_datasets\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    168\u001b[39m     train=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m    169\u001b[39m     transform=\u001b[38;5;28mself\u001b[39m.transform, \n\u001b[32m    170\u001b[39m     download=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28mself\u001b[39m.test_dataset = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDatasets/raw_datasets\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.num_classes = \u001b[38;5;28mlen\u001b[39m(np.unique(\u001b[38;5;28mself\u001b[39m.train_dataset.targets))\n\u001b[32m    180\u001b[39m \u001b[38;5;28mself\u001b[39m.train_num_samples = \u001b[38;5;28mself\u001b[39m.train_dataset.data.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:66\u001b[39m, in \u001b[36mCIFAR10.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.train = train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:137\u001b[39m, in \u001b[36mCIFAR10.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    139\u001b[39m     download_and_extract_archive(\u001b[38;5;28mself\u001b[39m.url, \u001b[38;5;28mself\u001b[39m.root, filename=\u001b[38;5;28mself\u001b[39m.filename, md5=\u001b[38;5;28mself\u001b[39m.tgz_md5)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:132\u001b[39m, in \u001b[36mCIFAR10._check_integrity\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m filename, md5 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_list + \u001b[38;5;28mself\u001b[39m.test_list:\n\u001b[32m    131\u001b[39m     fpath = os.path.join(\u001b[38;5;28mself\u001b[39m.root, \u001b[38;5;28mself\u001b[39m.base_folder, filename)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/utils.py:58\u001b[39m, in \u001b[36mcheck_integrity\u001b[39m\u001b[34m(fpath, md5)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m md5 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_md5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/utils.py:50\u001b[39m, in \u001b[36mcheck_md5\u001b[39m\u001b[34m(fpath, md5, **kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_md5\u001b[39m(fpath: Union[\u001b[38;5;28mstr\u001b[39m, pathlib.Path], md5: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m md5 == \u001b[43mcalculate_md5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/torchvision/datasets/utils.py:44\u001b[39m, in \u001b[36mcalculate_md5\u001b[39m\u001b[34m(fpath, chunk_size)\u001b[39m\n\u001b[32m     42\u001b[39m     md5 = hashlib.md5()\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fpath, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m chunk := \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     45\u001b[39m         md5.update(chunk)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m md5.hexdigest()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset_name = 'Cifar10'\n",
    "Data = Data_handling(\n",
    "    # dataset='mnist',\n",
    "    dataset=dataset_name,\n",
    "    train_size=0.8,\n",
    "    test_size=0.2,\n",
    "    batch_size=64,\n",
    "    shuffling=False,\n",
    "    splitting_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ebde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(50000, 3, 32, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m Data.train_dataset.targets \u001b[38;5;66;03m# This is Train_y\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(Data.test_dataset.targets)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mData\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfeature_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mData\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m] = [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m Data.train_dataset.targets.numpy()]\n\u001b[32m      6\u001b[39m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/pandas/core/frame.py:827\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    816\u001b[39m         mgr = dict_to_mgr(\n\u001b[32m    817\u001b[39m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[32m    818\u001b[39m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    824\u001b[39m             copy=_copy,\n\u001b[32m    825\u001b[39m         )\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:314\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    313\u001b[39m         values = np.asarray(values)\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     values = \u001b[43m_ensure_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[32m    319\u001b[39m     values = _prep_ndarraylike(values, copy=copy_on_sanitize)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Weak_MC/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:592\u001b[39m, in \u001b[36m_ensure_2d\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    590\u001b[39m     values = values.reshape((values.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m))\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m values.ndim != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[31mValueError\u001b[39m: Must pass 2-d input. shape=(50000, 3, 32, 32)"
     ]
    }
   ],
   "source": [
    "Data.train_dataset.data # This is Train_X\n",
    "Data.train_dataset.targets # This is Train_y\n",
    "print(Data.test_dataset.targets)\n",
    "df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "df['target'] = [i for i in Data.train_dataset.targets.numpy()]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b001a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e126947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df_2_plot = df.iloc[0:1000]\n",
    "features = ['feature_102', 'feature_103']\n",
    "visualize_dataset(\n",
    "    df_2_plot,\n",
    "    features=features,\n",
    "    classes=Data.num_classes,\n",
    "    title=dataset_name,\n",
    ") \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df_2_plot[[features[0], features[1]]] \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27509ea8",
   "metadata": {},
   "source": [
    "Next, we’ll simulate a **partial‐label learning** or **noisy-label** setting by corrupting each true label with **M**:\n",
    "\n",
    "1. **Instantiate** a `Weakener` with the number of true classes.  \n",
    "2. **Build** a mixing matrix via `generate_M(model_class='pll', corr_p=…)` \n",
    "3. **Generate** weak labels with `generate_weak`, which returns:\n",
    "   - `z`: the integer index of the weak‐label   \n",
    "   - `w`: a binary matrix of shape `(n_samples, n_classes)` indicating the candidate labels  \n",
    "4. **Insert** the partial labels into our Data using `include_weak(w)`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587776a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated M matrix:\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.34217728e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  1.34217728e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  3.35544320e-02 3.35544320e-02]\n",
      " ...\n",
      " [2.04800000e-06 2.04800000e-06 2.04800000e-06 ... 2.04800000e-06\n",
      "  0.00000000e+00 2.04800000e-06]\n",
      " [2.04800000e-06 2.04800000e-06 2.04800000e-06 ... 2.04800000e-06\n",
      "  2.04800000e-06 0.00000000e+00]\n",
      " [5.12000000e-07 5.12000000e-07 5.12000000e-07 ... 5.12000000e-07\n",
      "  5.12000000e-07 5.12000000e-07]]\n",
      "Generated z (noisy labels):\n",
      "tensor([136, 274, 162,  ...,   8, 767, 320], dtype=torch.int32)\n",
      "Inputs batch shape: torch.Size([64, 3, 32, 32])\n",
      "Weak (partial) labels shape: torch.Size([64])\n",
      "True one-hot labels shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "corr_p = 0.2\n",
    "weakener = Weakener(true_classes=Data.num_classes)\n",
    "weakener.generate_M(model_class='pll', corr_p=0.2)\n",
    "# weakener.generate_M(model_class='unif_noise', corr_p=0.5) #Try this for noisy labels\n",
    "print(f\"Generated M matrix:\\n{weakener.M}\")\n",
    "true_onehot = Data.train_dataset.targets  # shape: (n_samples, n_classes)\n",
    "\n",
    "z = weakener.generate_weak(true_onehot)\n",
    "print(f\"Generated z (noisy labels):\\n{z}\")\n",
    "#print(f\"Generated w (multi-label matrix):\\n{w}\")\n",
    "\n",
    "Data.include_weak(z)\n",
    "\n",
    "train_loader, test_loader = Data.get_dataloader(weak_labels='weak')\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "xb, wb, yb = batch\n",
    "print(f\"Inputs batch shape: {xb.shape}\")\n",
    "print(f\"Weak (partial) labels shape: {wb.shape}\")\n",
    "print(f\"True one-hot labels shape: {yb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69c4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>weak_0</th>\n",
       "      <th>weak_1</th>\n",
       "      <th>weak_2</th>\n",
       "      <th>weak_3</th>\n",
       "      <th>weak_4</th>\n",
       "      <th>weak_5</th>\n",
       "      <th>weak_6</th>\n",
       "      <th>weak_7</th>\n",
       "      <th>weak_8</th>\n",
       "      <th>weak_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.537255</td>\n",
       "      <td>-0.662745</td>\n",
       "      <td>-0.607843</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.231373</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.168628</td>\n",
       "      <td>0.168628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.207843</td>\n",
       "      <td>-0.011765</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.019608</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.349020</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.113726</td>\n",
       "      <td>-0.129412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.780392</td>\n",
       "      <td>-0.709804</td>\n",
       "      <td>-0.701961</td>\n",
       "      <td>-0.670588</td>\n",
       "      <td>-0.654902</td>\n",
       "      <td>-0.686275</td>\n",
       "      <td>-0.686275</td>\n",
       "      <td>-0.811765</td>\n",
       "      <td>-0.749020</td>\n",
       "      <td>-0.662745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.443137</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.231373</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>-0.725490</td>\n",
       "      <td>-0.686275</td>\n",
       "      <td>-0.670588</td>\n",
       "      <td>-0.694118</td>\n",
       "      <td>-0.662745</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-0.623529</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.584314</td>\n",
       "      <td>-0.560784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.443137</td>\n",
       "      <td>0.427451</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.898039</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.898039</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3083 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0      -0.537255  -0.662745  -0.607843  -0.466667  -0.231373  -0.066667   \n",
       "1       0.207843  -0.011765  -0.176471  -0.200000  -0.019608   0.215686   \n",
       "2       1.000000   0.984314   0.984314   0.984314   0.984314   0.984314   \n",
       "3      -0.780392  -0.709804  -0.701961  -0.670588  -0.654902  -0.686275   \n",
       "4       0.333333   0.317647   0.388235   0.435294   0.419608   0.388235   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "49995   0.137255   0.184314   0.223529   0.215686   0.200000   0.200000   \n",
       "49996   1.000000   0.992157   1.000000   0.992157   0.992157   0.992157   \n",
       "49997  -0.725490  -0.686275  -0.670588  -0.694118  -0.662745  -0.647059   \n",
       "49998   0.482353   0.458824   0.450980   0.443137   0.427451   0.419608   \n",
       "49999   0.796078   0.850980   0.835294   0.811765   0.850980   0.898039   \n",
       "\n",
       "       feature_6  feature_7  feature_8  feature_9  ...  weak_0  weak_1  \\\n",
       "0       0.090196   0.137255   0.168628   0.168628  ...     0.0     0.0   \n",
       "1       0.349020   0.411765   0.113726  -0.129412  ...     0.0     1.0   \n",
       "2       0.984314   0.984314   0.984314   0.984314  ...     0.0     0.0   \n",
       "3      -0.686275  -0.811765  -0.749020  -0.662745  ...     0.0     0.0   \n",
       "4       0.419608   0.443137   0.482353   0.482353  ...     0.0     1.0   \n",
       "...          ...        ...        ...        ...  ...     ...     ...   \n",
       "49995   0.231373   0.262745   0.262745   0.262745  ...     0.0     0.0   \n",
       "49996   0.992157   0.984314   0.992157   0.992157  ...     0.0     1.0   \n",
       "49997  -0.623529  -0.600000  -0.584314  -0.560784  ...     0.0     0.0   \n",
       "49998   0.411765   0.411765   0.419608   0.411765  ...     1.0     1.0   \n",
       "49999   0.929412   0.929412   0.898039   0.850980  ...     0.0     1.0   \n",
       "\n",
       "       weak_2  weak_3  weak_4  weak_5  weak_6  weak_7  weak_8  weak_9  \n",
       "0         1.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0  \n",
       "1         0.0     0.0     0.0     1.0     0.0     0.0     1.0     1.0  \n",
       "2         1.0     0.0     1.0     0.0     0.0     0.0     1.0     1.0  \n",
       "3         1.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0  \n",
       "4         0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0  \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "49995     1.0     1.0     0.0     0.0     0.0     0.0     0.0     1.0  \n",
       "49996     0.0     0.0     0.0     0.0     1.0     1.0     0.0     0.0  \n",
       "49997     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0  \n",
       "49998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "49999     0.0     1.0     0.0     0.0     0.0     0.0     0.0     1.0  \n",
       "\n",
       "[50000 rows x 3083 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weak_df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "#df['target'] = [i for i in weakener.w.numpy()]\n",
    "#df\n",
    "\n",
    "# 1) 展平成 (N, 3072)\n",
    "X = Data.train_dataset.data                # (N, 3, 32, 32)  (torch tensor)\n",
    "X2 = X.view(X.shape[0], -1).cpu().numpy()  # (N, 3072)\n",
    "\n",
    "weak_df = pd.DataFrame(X2, columns=[f'feature_{i}' for i in range(X2.shape[1])])\n",
    "\n",
    "# 2) 加 true label（如果 targets 是 one-hot，就转成 class index）\n",
    "y = Data.train_dataset.targets\n",
    "if hasattr(y, \"ndim\") and y.ndim == 2:\n",
    "    y = y.argmax(dim=1)\n",
    "weak_df[\"target\"] = y.cpu().numpy()\n",
    "\n",
    "# 3) 加 weak label（weakener.w 可能是一维或二维：做个兼容）\n",
    "w = weakener.w\n",
    "w_np = w.detach().cpu().numpy()\n",
    "\n",
    "if w_np.ndim == 1:\n",
    "    weak_df[\"weak\"] = w_np\n",
    "else:\n",
    "    # 如果是 one-hot / multi-hot (N,C)，你可以：\n",
    "    # A) 每一类一列（适合做统计）\n",
    "    for c in range(w_np.shape[1]):\n",
    "        weak_df[f\"weak_{c}\"] = w_np[:, c]\n",
    "    # 或 B) 压缩成“候选集合”（适合阅读）\n",
    "    # weak_df[\"weak_set\"] = [np.flatnonzero(row).tolist() for row in w_np]\n",
    "\n",
    "weak_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_dataset(\n",
    "#     df,\n",
    "#     features=['feature_0', 'feature_1'],\n",
    "#     classes=3,\n",
    "#     title='Iris Samples with Pie Markers for Multi-Label Entries'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53b540",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Instantiate** the model (e.g. `MLP`) with its input/output dimensions.   \n",
    "2. **Choose** the optimizer and set hyperparameters.  \n",
    "3. **Define** the loss function.\n",
    "\n",
    "We also could do a learning rate scheduler (e.g. `StepLR`) to decrease the LR over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8ac2a",
   "metadata": {},
   "source": [
    "## Training the MLP (using `train_test_loop.py`)\n",
    "\n",
    "1. **Set** training hyperparameters  \n",
    "2. **Call** `train_and_evaluate(model, train_loader, test_loader, optimizer, pll_loss, num_epochs, corr_p)`\n",
    "3. **Plot** results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fac0280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "   epoch  train_loss  train_acc  test_acc  train_detached_loss  \\\n",
      "0      1    0.092617    0.16248    0.2574             0.030664   \n",
      "\n",
      "   test_detached_loss optimizer loss_fn repetition  initial_lr  actual_lr  \\\n",
      "0            0.030788       SGD    None       None         0.1        0.1   \n",
      "\n",
      "   corr_p  epoch_time  \n",
      "0     0.2   1277.0138  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKYAAAHqCAYAAAA+vEZWAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaapJREFUeJzt3QmcjWX7wPHLMGOMnbGNbYjsS9aIVPZUZEnyZkm0yVaKLGNJlpCKCI3qzZYsqSRD5O1FE4OoLFFkHV5ZBzPNnP/nuvuc8z9n5swYzDzPzJzf9/M5zZznPOc5z3NPzOV6rvu6szkcDocAAAAAAAAAFvOz+gMBAAAAAAAARWIKAAAAAAAAtiAxBQAAAAAAAFuQmAIAAAAAAIAtSEwBAAAAAADAFiSmAAAAAAAAYAsSUwAAAAAAALAFiSkAAAAAAADYgsQUAAAAAAAAbEFiCgAAAAAAALYgMQUgQ/jwww8lW7Zssn37dskMdu3aJf/617+kdOnSkjNnTilUqJC0aNFCFixYIPHx8XafHgAAsNl7771nYpuGDRvafSqZ0unTp+Xll1+WypUrS1BQkOTOnVvq1q0rr7/+upw/f97u0wOQhnKk5cEAwBfMnz9fnn32WSlWrJg8+eSTUrFiRbl06ZJs2LBB+vTpIydPnpTXXnvN7tMEAAA2WrhwoYSGhkpkZKT89ttvUqFCBbtPKdP48ccf5cEHH5TLly+bG4GakFJ6A3PSpEmyefNmWbdund2nCSCNkJgCgJuwbds2k5Rq1KiRrFmzRvLmzet6bdCgQSZg2rt3b5p81pUrV8zdQQAAkLn8/vvvsmXLFlmxYoU888wzJkkVFhYmGVFGize0GurRRx+V7Nmzy86dO03FlLsJEybIvHnzsuS1A76KqXwAMhUNUNq2bSv58uWTPHnySPPmzU2yyF1cXJyMHTvWVDIFBgZK4cKFpUmTJhIREeHa59SpU9K7d28pVaqUmYpXokQJad++vfzxxx8pfr4eV8vyNcB0T0o51atXT3r16mW+37Rpk9lXv7rTz9DtOn3RSd+j13Po0CFzh1CP3b17d+nfv7/ZHhMTk+SzunXrJsWLF/eYOvj1119L06ZNTZClx2jXrp38/PPPqRpbAACQNjROKFiwoPk93LlzZ/M8uSTM4MGDTWWVxiMal/To0UPOnj3r2ufatWsyZswYufPOO01cozFLx44dTcyQVvGG+s9//iNdunSRMmXKmHPRdgV6blevXk1y3vv27ZPHHntMihQpIrly5ZJKlSrJiBEjzGsbN240n7ty5cok71u0aJF5bevWrcmO3fvvvy/Hjx+X6dOnJ0lKKa1YHzlypOu5Hk/HJzEdU2dM5t424rvvvpPnn39eihYtasb7s88+c233di76mvtNR712/ZlqGwf9eWjst3r16puORQH8PyqmAGQammDRpIsmpV555RXx9/c3AcN9991ngglnDwcNTiZOnChPP/20NGjQQC5evGgqmaKioqRly5Zmn06dOpnjvfjiiyZwiY6ONsHC0aNHzXNvNDmk0/XuvfdeE7Sltb///ltat25tApepU6eafgp6LrNmzZKvvvrKBIvu5/LFF1+YgEvvKKp///vf0rNnT3OMyZMnm31mz55tjqcJveSuCwAApC1NRGnyKCAgwNxI0t/HOj2tfv36rn10mprGNb/++qs89dRTUqdOHZOQ0iTHsWPHJDg42Nx8euihh0z88fjjj8vAgQNN+wCNWTRZcscdd6RJvKGWLVtmYofnnnvOJFJ0CuK7775rzkVfc/rpp5/MeWsc1q9fPxNfaKJL4xKtZtK4TJNaOgZa+ZR4XPSctfI8OXr9muzS5E960KSUJtRGjx5tKqY0eajJuk8//VSaNWvmse/SpUulWrVqUr16dfNcY8d77rlHSpYsKcOGDTM3AvV9HTp0kOXLl7uuNzWxKAA3DgDIABYsWODQv5J+/PHHZPfp0KGDIyAgwHHo0CHXthMnTjjy5s3ruPfee13batWq5WjXrl2yx/nrr7/MZ7355ps3dY67d+827xs4cGCq9t+4caPZX7+6+/333812vWannj17mm3Dhg3z2DchIcFRsmRJR6dOnTy2f/rpp2b/zZs3m+eXLl1yFChQwNG3b1+P/U6dOuXInz9/ku0AACB9bN++3fyOjoiIcP0uL1WqVJL4YfTo0Wa/FStWJDmGvkeFh4ebfaZPn57sPmkRb6iYmJgk2yZOnOjIli2b48iRI65tGnNp7OW+zf181PDhwx05c+Z0nD9/3rUtOjrakSNHDkdYWJgjJQULFjSxXGrp9Xg7ZtmyZc31Jo41mzRp4vj777899u3WrZujaNGiHttPnjzp8PPzc4wbN861rXnz5o4aNWo4rl275nHdjRs3dlSsWDHVsSgAT0zlA5Ap6B1DbXKpd6TKly/v2q7l7E888YR8//335m6UKlCggLmjdfDgQa/H0rtwegdTS97/+uuvVJ+D8/jepvClFb1L6U7Lx7VSSvtZ6Z1V9zt4erdO73YqvXOq0wH0rqzebXU+tJpKK8m0rB4AAKQ/rQrS6Wb333+/63d5165dZcmSJR7T77XCplatWkmqipzvce6jlVNa4Z3cPmkRbzjjIyetJNI4onHjxlrIYCqv1ZkzZ0zjca3wSlw97n4+Oh3x+vXrZpqce+yi1VrazPxG8VZ6xlp9+/Z1VZs76c9Hq+fdp0PquSckJJjX1Llz5+Tbb781Uxi1as0Za/3vf/8zFWgad+oUxNTEogA8kZgCkCloIKTl5drDILEqVaqYwOHPP/80z8eNG2eSNNqLoUaNGjJ06FBTdu6kfRN0qpv2Y9LAUafmTZkyxfSdSolOIVQajKSHHDlymF4HiWlApP0dnP0LNEGliSpNWDmDQGfg88ADD5jydPeHJvQ02AIAAOlLE0+agNKklDZA19X49KE3iU6fPm2m5Dnp9DfnFLHk6D4a+2iMkN7xhrYz0BYB2jtJp7ZpDOGc2nbhwgXz9fDhw+brjc5be0PptEX33lr6/d13333D1Qk13kqvWEuVK1cuybY2bdpI/vz5TfLMSb+vXbu2iSeV/hw1STdq1KgksZazsb0z3rpRLArAEz2mAGQ5mmjSQO7zzz83SZn58+fLW2+9JXPmzDFz/Z0r6D388MOyatUq+eabb0yQob0A9E7YXXfd5fW4GkhpMLdnz55UnUdydzLd75a604SZn1/S+wUaxGn/Bu1hoNVh2sNBE1XOO3hKE3POPlPaED2xtAxoAQCAdxpHnDx50iSn9JGYJmdatWqVpp+ZFvGG7qu9j7Qq6NVXXzWJJe2fpBVAmqxyxhk3Q6umtCeW9qjS6ildrGbmzJk3fJ9+9q5duyQ2NtZUuN+q5K7fvTLMfUy0Kl8btr/33nsmifjf//5X3njjDdc+zjF4+eWXTYWUN86kW2piUQD/j3+pAMgU9G6UNufcv39/ktd0dRQNsLTRppPe7dNV9/ShFUYaIGgjSvdgQJtvvvTSS+ahFUd6V2zatGnyySefeD0H/XytSNKgU6uz3D/PG12NR+kdM3dHjhy56evXsvG3337blLfrHTxNVGnCyv1alK4w06JFi5s+PgAAuH2aeNLfxbpwSWIrVqwwiQ9NTmhyRH93u6/25o3u88MPP5hV3rTZeHrFG3rT7cCBA/LRRx+ZhJJT4lXknO0UbnTeSpu1DxkyRBYvXmxuqOn5u99US47eONRV+3Qao7YouBG9/sTXrkktTRDeDD03vX6tatOG9Fod5X6+zmvX60hNrJWaWBTAP5jKByBT0F4AeodR7zzp8sdOekdLlx7WXkvOqXY619+dlqPrHSy9W6d0SqAuvZw48NN+Bs59kqOl2hqoPPnkkx49n5x27NhhghpVtmxZc97ai8Gd3om7WRoY6bnpsdeuXWsSVe70zp1ev97Z0+DV21RIAACQfjT5osknXUVPV5RL/Ojfv7+Zouacmq8rBO/evdskqxL7p6f3P/toHyNvlUbOfdIi3nD2XHIe0/m93hRLfKNQEyzh4eFm6p+383HS3lht27Y1N/w0YafT5XTbjTz77LOmh6jeONRkWWI6Xe7111/3iOESX/vcuXOTrZhKjiabNJmkNwD1oavpuU/704SjrjioK0J7S3q5x1o3ikUBeKJiCkCGooGOJl4S01JwDUL0zp0moXSpX52epsGB/pLXHlFOVatWNYFD3bp1TYChy/NqA0sNCJUGOc2bNzfJHd1Xj6NBoSa59O5eSrQJqN4F1c/XUnNNUFWsWNEEmtowU4NNZ7CkvQq0D5Qutaxl9ho4ffnll7fU70mXkNaAZsSIEeZ6E99x1KSULkWt56P76nVo8KhB41dffWWWNk5N+TwAALg1GgNoPPDII494fV0rnfV3syZp9Pe49h3S+ERjBW0mrnGLTqXT42hVlTZG1+qljz/+2FQeRUZGStOmTU1j8vXr15tYpH379mkSb2hMo+/TaWo6fU/jCq1Y8rZIzDvvvGNiMY03+vXrZ5I3etNQ4w2dgudOz1+Tcmr8+PGpOhetgNK47MEHHzTV7NosXcdGRUVFmQqsRo0aufbXCiRNZmkST6cjarJP2zSkJgnmTiuhOnbsaKZg6hhPnTo1yT4aA+q1a98obaKuVVQaP2qFl05Z1M9OTSwKIJFEq/QBgC2cS/gm9/jzzz/NflFRUY7WrVs78uTJ4wgKCnLcf//9ji1btngc6/XXX3c0aNDAUaBAAUeuXLkclStXdkyYMMERGxtrXj979qzjhRdeMNtz587tyJ8/v6Nhw4aOTz/9NNXnu2PHDscTTzzhCAkJcfj7+5uljXUJ4Y8++sgRHx/v2u/MmTOOTp06mXPVfZ555hnH3r17vS7frOeSkhEjRpj3VahQIdl9dKloHR+9psDAQMcdd9zh6NWrl1m6GgAApJ+HH37Y/O69cuVKsvvo72SNGzQWUf/73/8c/fv3d5QsWdIREBDgKFWqlIkJnK+rmJgYEwOUK1fOvLd48eKOzp07Ow4dOpSm8cYvv/ziaNGihYmxgoODHX379nXs3r07yTGUHvvRRx81sZZec6VKlRyjRo1Kcszr16+b89G45OrVqzc1nidOnHAMHjzYceedd5rP0GurW7euiekuXLjg2k/jrldffdWcs+6jcdBvv/3mKFu2rLnexLHmjz/+mOxnRkREmH2yZcvmij0T03Hv0aOH+Tnoz0N/dg899JDjs88+S3UsCsBTNv1P4mQVAAAAAAC34++//5aQkBDTN+qDDz6w+3QAZFD0mAIAAAAApDld/Vh7L7k3VAeAxKiYAgAAAACkGV1J8KeffjJ9pbTXk/aGAoDkUDEFAAAAAEgzuiDLc889Z1ay0+btAJASKqYAAAAAAABgCyqmAAAAAAAAYAsSUwAAAAAAALBFDns+NvNLSEiQEydOSN68eSVbtmx2nw4AALCIdkG4dOmSWQLdz497fCkhXgIAwDc5biJeIjF1izTIKl26tN2nAQAAbPLnn39KqVKl7D6NDI14CQAA3/ZnKuIlElO3SO/8OQc5X758dp9OhhEXFyfr1q2TVq1aib+/v92n4xMYc+sx5vZg3K3HmHt38eJFk2xxxgJIHvGSd/zZsh5jbg/G3XqMufUY89uPl0hM3SJnOboGWQRann8og4KCzJjwh9IajLn1GHN7MO7WY8xTxtS0GyNe8o4/W9ZjzO3BuFuPMbceY3778RKNEQAAAAAAAGALElMAAABZwKxZsyQ0NFQCAwOlYcOGEhkZmey+8+bNk6ZNm0rBggXNo0WLFl73//XXX+WRRx6R/PnzS+7cuaV+/fpy9OjRdL4SAADgS0hMAQAAZHJLly6VIUOGSFhYmERFRUmtWrWkdevWEh0d7XX/TZs2Sbdu3WTjxo2ydetW0wNCe2McP37ctc+hQ4ekSZMmUrlyZbP/Tz/9JKNGjTKJLwAAgLRCjykAANJIfHy86TOQVei15MiRQ65du2auzVdof4js2bNLZjJ9+nTp27ev9O7d2zyfM2eOfPXVVxIeHi7Dhg1Lsv/ChQs9ns+fP1+WL18uGzZskB49ephtI0aMkAcffFCmTJni2u+OO+5I92sBAKSPhIQEiY2Ntfs0shzipdtHYgoAgNvkcDjk1KlTcv78eclq11W8eHGzopqvNfouUKCAufbMcN36j4wdO3bI8OHDXdv8/PzM9DythkqNmJgYE1gXKlTI9Y8XTWy98sorpvJq586dUq5cOfMZHTp0SLdrAQCk3++K33//3fz9jrRFvFT8tq+bxBQAALfJmZQqWrSoWZUlqwQlGrxevnxZ8uTJYxIdvhJcapLGOQWuRIkSktGdPXvW3KEtVqyYx3Z9vm/fvlQd49VXX5WQkBCTzFJ6/fqznzRpkrz++usyefJkWbt2rXTs2NFM/2vWrJnX41y/ft083JeKVpr0ykrVhLfLORaMiXUYc3sw7hljzPV3m07V1t/lJUuW9Jnf6VbR8b1y5YrpxZhVYsDUxktnzpzxGoPc7J97ElMAANwG/WXsTEoVLlxYsmLJv/YU8qUgNleuXK7kjP5cM9u0vpulyaclS5aYPlLO/lHOO+rt27eXwYMHm+9r164tW7ZsMdMEk0tMTZw4UcaOHZtk+7p160zSFp4iIiLsPgWfw5jbg3G3d8z1d7jeaNEbEH///bet55VVBQQE+FwC1t/fX/LmzSsnT540/S01WeVOE1epRWIKAIDb4AxC+Ed31uL8eerPN6MnpoKDg805nj592mO7Ptfy+pRMnTrVJKbWr18vNWvW9Dim9suoWrWqx/5VqlSR77//Ptnj6VQ/bcLuXjHlbKyeL1++W7i6rEn/v9J/NLZs2dIE9kh/jLk9GPeMMeZayaorquoKq86bL0g7mpC5dOmSSdL4SsWUk/4/ptf+wAMPSM6cOcWds2o6NUhMAQCQBnwtEMnqMtPPU+/S1q1b1zQud/Z/0oonfd6/f/9k36dNzSdMmCDffPON1KtXL8kx69evL/v37/fYfuDAASlbtmyyx9SgNHFg6gxc+UdpUoyL9RhzezDu9o65Vnfr7zW9ieFLFdBWcVYZ6xj72vhmz57dXLfezEr8Z/xm/syTmAIAAMjktEqpZ8+eJsHUoEEDmTFjhul34VylT1fa074iOtVOac+o0aNHy6JFiyQ0NNT0SVPaT0wfaujQodK1a1e599575f777zc9pr744gsz5Q8AACCt+FY6DwAApBtNcGhCBNbTBJJOy9Nkk/aC2rVrl0kkOZuR6hQO7QHhNHv2bNM/rHPnzqbviPOhx3B69NFHTT8prayqUaOGzJ8/X5YvXy5NmjSx5RoBALhdxCoZExVTAAD4mBtNUwsLC5MxY8bc9HF//PFHsyLN7bjvvvtMYoWg8ebptL3kpu4lrnL6448/UnXMp556yjwAALBSRo5VnBYvXiz/+te/5JlnnpE33ngjTY7pq0hMAQDgY9wrZ5YuXWqqbNx7CTmncjkbeuoKPtpz6EaKFCmSDmcLAAB8zc3GKtpHS/scWRmrfPDBB/LKK6/I+++/b87PzkU+YmNjUxWrZVRM5QMAwMfoSm3Oh67Qo3clnc/37dtnVpX5+uuvTfNrnQqmq7AdOnRI2rdvb55rMKiv6UpuKZXH63F1+pdOCdNV7ipWrCirV6++rXPXqWTVqlUzDbb186ZNm+bx+nvvvWc+JzAw0JyrTlVz+uyzz8yUNF2RqHDhwtKiRQvThwkAAGTOWEUX/9CYwOpY5ffff5ctW7bIsGHD5M477zQ9GBMLDw93xSw6Xd69qvn8+fOm0krPNTAwUKpXry5ffvmleU0rwbR63J2es567U69evcyCJ7qISUhIiFSqVMls//e//236Ter46Fg98cQTEh0d7XGsn3/+WR566CGTSNP9mjZtasZu8+bNpmG5s++k06BBg8w+6YnEFAAAaUjv2sXE/m3LQz87rWigpWXpP/zwg9SsWVMuX74sDz74oFnpbefOndKmTRt5+OGHTe+ilIwdO1Yee+wx+emnn8z7u3fvLufOnbulc9qxY4c51uOPPy579uwxgduoUaPkww8/NK9v375dBgwYIOPGjTN3VbXHkjbudt557datm5mW9uuvv5qpbR07dkzTMQMAIDPISrHKpEmTzO91q2OVBQsWSLt27UzSTPf/5JNPPF7XXo4vvPCC9OvXz8QsmuyqUKGCaxW/tm3byn//+1/zvl9++cVch65wdzP0OjXeiYiIcCW14uLiZPz48bJ7925ZtWqVmbqvSSyn48ePm9hIk2Xffvutia00NtLqeN1evnx5k9xy0uMtXLgw3af1M5UPAIA0dDUuXqqO/saWz/5lXGsJCkibX+2a3GnZsqVcvHjR3FELDg6WWrVquV7XoGflypUm0Equr5HSYEgTQkoTXe+8845ERkaaYPFmTZ8+XZo3b26SUUrvUGow9+abb5rP0cBT+0boXUC9A1i2bFm56667XIkpDbo0GaXblVZPAQDga7JarOJUqFAhS2IVTSzpTbF3333XtQDJyy+/bKqo7rjjDrPt9ddfl5deekkGDhzoep9WcCmt4tLja0JNYxmlCaGbpTGPVnu5T+FzTyDpMfVa9HM1aadVZLNmzTLJtCVLlpjqKOU8B9WnTx+TdNOVeZVWgl27ds0k7tITFVMAACAJLQN3pwGNBl1VqlSRAgUKmOBGA6ob3YXUO5juAZQmuRKXlKeWft4999zjsU2fHzx40PSW0OBUk04aiD355JPmDl9MTIzZTwNVTWppMqpLly4yb948+euvv27pPAAAgO/GKlqhpK0AtLpK6c07XbxFEzpK33vixAkTd3ijK+eWKlXKIyF0KzSmSdxXSiugtEqsTJky5iZds2bNzHbnGOhn67Q8Z1LKW5Lut99+k23btpnnmoDTpFRaNYxPDhVTAACkoVz+2c3dQLs+O60kDkA00NNAbOrUqaYUXfs0af8mbbaZksSBj/Zy0DuN6UEDsKioKDNNb926daYRqU730xV4NEDV89d+EPqa3uUcMWKEmapYrly5dDkfAAAyImKV24tVtOm5TvXT4zvp/poE0you9+3e3Oh1Pz+/JFMedUrdja5fk2WtW7c2D705p43eNSGlz51jcKPPLlq0qElsaZJN4yPt45V4Zd/0QGIKAIA0pMFMWpWoZyTaB0HvomlzUOddSe1bYCW9A6rnkfi89I6jsy+DrsijTc31oUtJa0JKeyjoFD792WiFlT40aaXVVVriP2TIEEuvAwAAOxGr3Lr//e9/8vnnn5upcNrY3JmUunDhgqmg0ptfOgVQG5VrD6j777/fa4XWsWPH5MCBA16rpooUKWIakGtySn9WzkqnG9Gm8Hp+2q+qdOnSrv6biT/7o48+Momu5Kqmnn76aTO1Uau6dGpi4mr19JD1/m8EAABpTlepWbFihbmLpkGS9nlKr8qnM2fOJAnAdDUb7dWgfRK0Z4T2c9i6davMnDnTrMSntPHn4cOHTfPOggULypo1a8w56ko1WhmlAWKrVq3M3UB9rp+jyS4AAJD5WRGraGNwXdlXp7c5k0b6GdqTUxuaazWVJqa0YvvZZ581MYduv3Tpkkmcvfjii2Z6ncYqnTp1Mv0zK1SoYJJKejx9r04L1BhlypQppuJLF3PRyiWdYpgSnb6nU/u0Klw/e+/evSZmcqe9tvR1XUhm+PDhpt+UTttr0KCBa2U/rbDSz9I+WVoBZgV6TAEAgBvSwEmTPY0bNzYBnwYtderUSZfPWrRokWla7v7QnlD6eZ9++qm5S6nLKmvVkwZMztVmtDpKA9IHHnjAJJzmzJkjixcvNnc0NcDSZZD1bqbenRw5cqRMmzbNBIsAACDzsyJWCQ8PNxVZzqSUO63O1kbrZ8+elZ49e8qMGTPMzTONQ3RhFu2J6bR8+XJzs00rk6pWrSqvvPKK6ZepNIbR92mjcu2RqY3SdZrijWillfaEWrZsmTmmVk7ptEZ3mlTTSnKtJtMEWd26dU2M5V49pVMJNbbS8+nRo4dYIZuDdZJviWZENbuoJXs3ylz6Ei0J1DvUGvgnVxqItMWYW48xt0dGHXddqURXYdF5+IGBgZKVOO8A6u85DVJ8SUo/V2KA1GOsMtffZ1kZY24Pxj1jjHlWjlUygqwYL/Xp08dUbWmizYp4ial8AAAAAAAAPu7ChQuyZ88eU71+o6RUWiIxBQAAAAAA4OPat29vpg5qj6qWLVta9rkkpgAAAAAAAHzcpk2bbPncrDEBEgAAAAAAAJkOiSkAAAAAAADYgsQUAABptCILsg5+ngAAANagxxQAALchICDALA184sQJKVKkiHmeLVs2ySrJmdjYWLMUcFZZ/vhGHA6HuWZdIlmvWX+eAAAASD8kpgAAuA2avChXrpycPHnSJKeyWpLm6tWrkitXriyTbEutoKAgKVOmjM8k5AAAAOxCYgoAgNukVTWaxPj7778lPj5esoq4uDjZvHmz3HvvveLv7y++Inv27JIjRw6fS8YBAADYgcQUAABpQJMYmrzJSgkcTdBosi0wMDBLXRcAAAAyDurTAQAAAABAhrrhl9JjzJgxt3XsVatWpXr/Z555xtysW7Zs2S1/JlJGxRQAAAAAAMgwtHen09KlS2X06NGyf/9+17Y8efJYch4xMTGyZMkSeeWVVyQ8PFy6dOkidoqNjc2SC7NQMQUAAAAAADKM4sWLux758+c3VU7u2zRZVKVKFdNuoHLlyvLee+95JG/69+8vJUqUMK+XLVtWJk6caF4LDQ01Xx999FFzTOfz5GiVVNWqVWXYsGGm7+aff/7p8fr169fNa9WqVTOLxVSoUEE++OAD1+s///yzPPTQQ5IvXz7JmzevNG3aVA4dOmReu++++2TQoEEex+vQoYP06tXL9VzPb/z48dKjRw9zjH79+pntr776qtx5551msZby5cvLqFGjTG9Qd1988YXUr1/fjEFwcLC5ZjVu3DipXr16kmutXbu2OY4dqJgCAAAAAMBXOBwicTH2fLZ/kM6lu61DLFy40FRQzZw5U+666y7ZuXOn9O3bV3Lnzi09e/aUd955R1avXi2ffvqpWZxGk0nOhNKPP/4oRYsWlQULFkibNm3MFL2UaJLpX//6l0mOtW3bVj788EOP5I0mjLZu3SqTJ0+Wu+++W44cOSJnz541rx0/ftwsIKMJqG+//dYklv773/+a/p03Y+rUqeZ6w8LCXNs0yaXnEhISInv27DHXr9u0skt99dVXJhE1YsQI+fjjj02ybs2aNea1p556SsaOHWvGQhNXSsfwp59+khUrVogdSEwBAAAAAOArNCn1Rog9n/3aCZGA3Ld1CE3QTJs2TTp27GielytXTn755Rd5//33TWLq6NGjUrFiRWnSpImpitKKKaciRYqYrwUKFDCVVyk5ePCgbNu2zZWs0QTVkCFDZOTIkea4Bw4cMMmvb775Rho0aGAST1ox5TRr1iyT0NLqLuciMlrldLMeeOABeemllzy26Tm4V1W9/PLLrimHasKECfL444+bBJRTrVq1zNdSpUpJ69atTXLOmZjS75s1a2aqr+zAVD4AAAAAAJDhXblyxUyF69Onj+kz5Xy8/vrrrilyOhVu165dUqlSJRkwYICsW7fulj5Le0ppAkenwakHH3xQLly4YKqflH6GVlxpQscbfV2n7t3uysb16tVLsk37bt1zzz0muabXr4kqTci5f3bz5s2TPaZWWC1evFiuXbtmqqkWLVpkKqnsQsUUAAAAAAC+QqfTaeWSXZ99Gy5fvmy+zps3Txo2bOjxmnNaXp06deT333+Xr7/+WtavXy+PPfaYtGjRQj777LNUf058fLx89NFHcurUKcmRI4fHdk1YadJHe0ql5Eav+/n5iUOnVbpJ3CdK6RRFdzp1sHv37qYaShNnzqosrSJL7Wc//PDDkjNnTlm5cqVppq6f27lzZ7ELiSkAAAAAAHyF9ni6zel0dilWrJjpq3T48GGTnEmOTqvr2rWreWjCRftJnTt3TgoVKmQqmDTBlBLtx3Tp0iXTe8m9D9XevXuld+/ecv78ealRo4YkJCTId999Z6byJVazZk2T3NKkj7eqKZ1W6L76YHx8vDn+/fffn+K5bdmyxUxP1P5RTtrbKvFnb9iwwZyrN5ps02mPOoVPE1M67e9Gyaz0RGIKAAAAAABkCloppFP0tFJIE066Mt727dvlr7/+Mj2gpk+fblbk08boWpWkK+vplDftK+XsyaRJG50Kp1VDBQsW9Nr0vF27dq6+TE66Qt/gwYNNA/YXXnjBJHeefvpps+qfNj/XJuvR0dGmSktXBnz33XdN0mf48OHmfLVnlSaxdJqh9o7S8/3qq6/kjjvuMOetCa8b0f5ZOm1Pq6S0R5S+XyufEvfh0qouPa5+vjZc12SbrubnpOetKxsqbcpuJ3pMAQAAAACATEETKvPnzzfVPlq1pD2edIU6bYKudHW6KVOmmN5Mmrj5448/TFJGk1RKp7xFRERI6dKlTfIqsdOnT5tkT6dOnZK8psfQ1e40caVmz55t9tPm45q00t5N2gdLFS5c2PSj0umHeo5169Y1UxCd1VPa00kTWz169HA1Hr9RtZR65JFHTHJME1+1a9c2FVTuKwUqXQlQE3K6OqHuo0mwyMjIJAmuxo0bS+XKlZNMi7RaNkfiSY1IlYsXL5qMpzY/0zJB/EPLFPUPvTaGu90mb0gdxtx6jLk9GHfrMebeEQOkHmPlHX+2rMeY24Nxzxhjrg2uteeSJm4CAwPtPsUsR6fz6e87/T3nTH5lBg6HwySnnn/+eVO5dStS+n/rZmIApvIBAAAAAAD4iDNnzpipgNrcPbk+VFYiMQUAAAAAAOAjihYtKsHBwTJ37lyvPbasRmIKAAAAAADARzgyWEenzDMBEgAAAAAAAFkKiSkAAAAAAADYgsQUAAAAAABZXEabvoWssSJhWqDHFAAAAAAAWZS/v79ky5bNrMRWpEgR8z3SNjkTGxsr165dEz8/P59JcsbGxpr/p/SaAwICbut4JKYAAAAAAMiismfPLqVKlZJjx47JH3/8YffpZMkkzdWrVyVXrlw+l/QLCgqSMmXK3HZCjsQUAAAAAABZWJ48eaRixYoSFxdn96lkOTqmmzdvlnvvvddUp/lSwjNHjhxpkowjMQUAAAAAgA8kEvSBtKVj+vfff0tgYKBPJabSkm9MgAQAAAAAAECGQ2IKAAAAAAAAtiAxBQAAAAAAAFuQmAIAAAAAAIAtSEwBAAAAAADAFiSmAAAAAAAAYAsSUwAAAAAAALAFiSkAAAAAAAD4ZmJq1qxZEhoaKoGBgdKwYUOJjIxMcf9ly5ZJ5cqVzf41atSQNWvWeLx++vRp6dWrl4SEhEhQUJC0adNGDh48mOQ4W7dulQceeEBy584t+fLlk3vvvVeuXr2a5tcHAAAAAACADJiYWrp0qQwZMkTCwsIkKipKatWqJa1bt5bo6Giv+2/ZskW6desmffr0kZ07d0qHDh3MY+/eveZ1h8Nhnh8+fFg+//xzs0/ZsmWlRYsWcuXKFY+klCasWrVqZRJhP/74o/Tv31/8/GzP0wEAAAAAAPgMWzMx06dPl759+0rv3r2latWqMmfOHFPlFB4e7nX/t99+2ySUhg4dKlWqVJHx48dLnTp1ZObMmeZ1rYzatm2bzJ49W+rXry+VKlUy32sl1OLFi13HGTx4sAwYMECGDRsm1apVM/s99thjkjNnTsuuHQAAAAAAwNflsOuDY2NjZceOHTJ8+HDXNq1Y0uomrWjyRrdrhZU7rbBatWqV+f769evmq07zcz+mJpy+//57efrpp0011g8//CDdu3eXxo0by6FDh8zUwAkTJkiTJk2SPV89tvP46uLFi+ZrXFyceeAfzrFgTKzDmFuPMbcH4249xtw7xgMAACALJKbOnj0r8fHxUqxYMY/t+nzfvn1e33Pq1Cmv++t2pQmmMmXKmGTX+++/b/pHvfXWW3Ls2DE5efKk2Uen+akxY8bI1KlTpXbt2vLxxx9L8+bNzZTAihUrev3siRMnytixY5NsX7dunanygqeIiAi7T8HnMObWY8ztwbhbjzH3FBMTY/cpAAAAZBm2JabSg7+/v6xYscL0oCpUqJBkz57dVGC1bdvW9J9SCQkJ5uszzzxjphCqu+66SzZs2GCmEGoCyhtNdrlXa2nFVOnSpU2fKm2ejv+/i6z/gGnZsqX5eSD9MebWY8ztwbhbjzH3zlk1DQAAgEycmAoODjaJI11Fz50+L168uNf36PYb7V+3bl3ZtWuXXLhwwUwXLFKkiFntr169eub1EiVKmK/a08qd9qw6evRosuer0wG99aDSQJ1gPSnGxXqMufUYc3sw7tZjzD0xFgAAAFmg+XlAQIBJImmlkpNWM+nzRo0aeX2PbnffX+mdXG/758+f3ySltCH69u3bpX379mZ7aGiohISEyP79+z32P3DggFnBDwAAAAAAAD4wlU+nxvXs2dNUMzVo0EBmzJghV65ccU2x69Gjh5QsWdI1vW7gwIHSrFkzmTZtmrRr106WLFlikk5z5851HXPZsmUmIaW9pvbs2WPe06FDBzPlTmXLls2s6hcWFia1atUyPaY++ugj09fqs88+s2kkAAAAAAAAfI+tiamuXbvKmTNnZPTo0aaBuSaJ1q5d62pwrlPrdFU9J11Fb9GiRTJy5Eh57bXXTKNyXZGvevXqrn20ybkmvHSKn07b0+TWqFGjPD530KBBcu3aNRk8eLCcO3fOJKi08uqOO+6w8OoBAAAAAAB8m+3Nz/v3728e3mzatCnJti5duphHcgYMGGAeNzJs2DDzAAAAAAAAgI/1mAIAAAAAAIBvIzEFAAAAAAAAW5CYAgAAAAAAgC1ITAEAAAAAAMAWJKYAAAAAAABgCxJTAAAAAAAAsAWJKQAAAAAAANiCxBQAAEAWMWvWLAkNDZXAwEBp2LChREZGJrvvvHnzpGnTplKwYEHzaNGiRYr7P/vss5ItWzaZMWNGOp09AADwRSSmAAAAsoClS5fKkCFDJCwsTKKioqRWrVrSunVriY6O9rr/pk2bpFu3brJx40bZunWrlC5dWlq1aiXHjx9Psu/KlStl27ZtEhISYsGVAAAAX0JiCgAAIAuYPn269O3bV3r37i1Vq1aVOXPmSFBQkISHh3vdf+HChfL8889L7dq1pXLlyjJ//nxJSEiQDRs2eOyniaoXX3zR7O/v72/R1QAAAF+Rw+4TAAAAwO2JjY2VHTt2yPDhw13b/Pz8zPQ8rYZKjZiYGImLi5NChQq5tmmi6sknn5ShQ4dKtWrVbniM69evm4fTxYsXzVc9rj7wD+dYMCbWYcztwbhbjzG3HmPu3c2MB4kpAACATO7s2bMSHx8vxYoV89iuz/ft25eqY7z66qtmqp4ms5wmT54sOXLkkAEDBqTqGBMnTpSxY8cm2b5u3TpTvQVPERERdp+Cz2HM7cG4W48xtx5jnvSGV2qRmAIAAPBxkyZNkiVLlpi+U9o4XWkF1ttvv236VWnT89TQii3tc+VeMeXsXZUvX750O//MeBdZ/wHTsmVLpkdahDG3B+NuPcbceoy5d86q6dQgMQUAAJDJBQcHS/bs2eX06dMe2/V58eLFU3zv1KlTTWJq/fr1UrNmTdf2//znP6ZxepkyZVzbtCrrpZdeMivz/fHHH0mOlTNnTvNITAN1gvWkGBfrMeb2YNytx5hbjzH3dDNjQfNzAACATC4gIEDq1q3r0bjc2ci8UaNGyb5vypQpMn78eFm7dq3Uq1fP4zXtLfXTTz/Jrl27XA+d6qf9pr755pt0vR4AAOA7qJgCAADIAnQKXc+ePU2CqUGDBqaq6cqVK2aVPtWjRw8pWbKk6QPl7B81evRoWbRokYSGhsqpU6fM9jx58phH4cKFzSPx3U+twKpUqZINVwgAALIiElMAAABZQNeuXeXMmTMm2aRJptq1a5tKKGdD9KNHj5qV+pxmz55tVvPr3Lmzx3HCwsJkzJgxlp8/AADwTSSmAAAAsoj+/fubhzfa2Nydtx5RN3Ir7wEAAEgJPaYAAAAAAABgCxJTAAAAAAAAsAWJKQAAAAAAANiCxBQAAAAAAABsQWIKAAAAAAAAtiAxBQAAAAAAAFuQmAIAAAAAAIAtSEwBAAAAAADAFiSmAAAAAAAAYAsSUwAAAAAAALAFiSkAAAAAAADYgsQUAAAAAAAAbEFiCgAAAAAAALYgMQUAAAAAAABbkJgCAAAAAACALUhMAQAAAAAAwBYkpgAAAAAAAGALElMAAAAAAACwBYkpAAAAAAAA2ILEFAAAAAAAAGxBYgoAAAAAAAC2IDEFAAAAAAAAW5CYAgAAAAAAgC1ITAEAAAAAAMAWJKYAAAAAAABgCxJTAAAAAAAAsAWJKQAAAAAAANiCxBQAAAAAAABsQWIKAAAAAAAAtiAxBQAAAAAAAFuQmAIAAAAAAIAtSEwBAAAAAADAFiSmAAAAAAAAYAsSUwAAAAAAALAFiSkAAAAAAADYgsQUAAAAAAAAbEFiCgAAAAAAALYgMQUAAAAAAABbkJgCAAAAAACALUhMAQAAAAAAwBYkpgAAAAAAAGALElMAAAAAAACwBYkpAAAAAAAA2ILEFAAAAAAAAGxBYgoAAAAAAAC2IDEFAAAAAAAAW5CYAgAAAAAAgC1ITAEAAAAAAMAWJKYAAAAAAABgCxJTAAAAAAAA8N3E1KxZsyQ0NFQCAwOlYcOGEhkZmeL+y5Ytk8qVK5v9a9SoIWvWrPF4/fTp09KrVy8JCQmRoKAgadOmjRw8eNDrsRwOh7Rt21ayZcsmq1atStPrAgAAAAAAQAZOTC1dulSGDBkiYWFhEhUVJbVq1ZLWrVtLdHS01/23bNki3bp1kz59+sjOnTulQ4cO5rF3715XokmfHz58WD7//HOzT9myZaVFixZy5cqVJMebMWOGSUoBAAAAAADAxxJT06dPl759+0rv3r2latWqMmfOHFPlFB4e7nX/t99+21RADR06VKpUqSLjx4+XOnXqyMyZM83rWhm1bds2mT17ttSvX18qVapkvr969aosXrzY41i7du2SadOmJftZAAAAAAAAyKKJqdjYWNmxY4epZnKdkJ+feb5161av79Ht7vsrrbBy7n/9+nXzVaf5uR8zZ86c8v3337u2xcTEyBNPPGGmERYvXjzNrw0AAAAAAAApyyE2Onv2rMTHx0uxYsU8tuvzffv2eX3PqVOnvO6v25X2nipTpowMHz5c3n//fcmdO7e89dZbcuzYMTl58qTrPYMHD5bGjRtL+/btU3WumvByJr3UxYsXzde4uDjzwD+cY8GYWIcxtx5jbg/G3XqMuXeMBwAAQBZJTKUHf39/WbFihelBVahQIcmePbupsNIG59p/Sq1evVq+/fZb038qtSZOnChjx45Nsn3dunVm6iE8RURE2H0KPocxtx5jbg/G3XqMuSetugYAAEAWSEwFBwebxJGuoudOnyc3vU6332j/unXrmv5RFy5cMNMFixQpYlb7q1evnnldk1KHDh2SAgUKeBynU6dO0rRpU9m0aVOSz9UKLG3S7l4xVbp0aWnVqpXky5fvFkcga95F1n/AtGzZ0iQJkf4Yc+sx5vZg3K3HmHvnrJoGAABAJk9MBQQEmCTShg0bzEp6KiEhwTzv37+/1/c0atTIvD5o0CDXNg2adXti+fPndzVE3759u2mUroYNGyZPP/20x741atQwU/4efvhhr5+rPar0kZgG6gTrSTEu1mPMrceY24Nxtx5j7omxAAAAyEJT+bQKqWfPnqaaqUGDBjJjxgy5cuWKWaVP9ejRQ0qWLGmm0qmBAwdKs2bNzGp67dq1kyVLlpik09y5c13HXLZsmamS0l5Te/bsMe/RxJdWNymtrvJWkaX7lytXzrJrBwAAAAAA8GW2J6a6du0qZ86ckdGjR5sG5rVr15a1a9e6GpwfPXrUrKrnpA3LFy1aJCNHjpTXXntNKlasKKtWrZLq1au79tEm55rw0il+JUqUMMmtUaNG2XJ9AAAAAAAAyKCJKaXT9pKbuuet31OXLl3MIzkDBgwwj5vhbIwOAAAAAAAAa/x/KRIAAAAAAABgIRJTAAAAAAAAsAWJKQAAAAAAANiCxBQAAAAAAABsQWIKAAAAAAAAtiAxBQAAkEXMmjVLQkNDJTAwUBo2bCiRkZHJ7jtv3jxp2rSpFCxY0DxatGjhsX9cXJy8+uqrUqNGDcmdO7eEhIRIjx495MSJExZdDQAA8AUkpgAAALKApUuXypAhQyQsLEyioqKkVq1a0rp1a4mOjva6/6ZNm6Rbt26yceNG2bp1q5QuXVpatWolx48fN6/HxMSY44waNcp8XbFihezfv18eeeQRi68MAABkZTnsPgEAAADcvunTp0vfvn2ld+/e5vmcOXPkq6++kvDwcBk2bFiS/RcuXOjxfP78+bJ8+XLZsGGDqYzKnz+/REREeOwzc+ZMadCggRw9elTKlCmTzlcEAAB8ARVTAAAAmVxsbKzs2LHDTMdz8vPzM8+1Gio1tEJKp+8VKlQo2X0uXLgg2bJlkwIFCqTJeQMAAFAxBQAAkMmdPXtW4uPjpVixYh7b9fm+fftSdQztJ6V9pNyTW+6uXbtm9tHpf/ny5fO6z/Xr183D6eLFi+arJrz0gX84x4IxsQ5jbg/G3XqMufUYc+9uZjxITAEAAPi4SZMmyZIlS0zfKW2c7i24fOyxx8ThcMjs2bOTPc7EiRNl7NixSbavW7dOgoKC0vy8M7vEUyWR/hhzezDu1mPMrceYJ63ETi0SUwAAAJlccHCwZM+eXU6fPu2xXZ8XL148xfdOnTrVJKbWr18vNWvWTDYpdeTIEfn222+TrZZSw4cPNw3Y3SumnE3VU3qfr9Ex1X/AtGzZUvz9/e0+HZ/AmNuDcbceY249xtw7Z9V0apCYAgAAyOQCAgKkbt26pnF5hw4dzLaEhATzvH///sm+b8qUKTJhwgT55ptvpF69eskmpQ4ePGhW7ytcuHCK55EzZ07zSEwDdYL1pBgX6zHm9mDcrceYW48x93QzY0FiCgAAIAvQSqWePXuaBJOunDdjxgy5cuWKa5U+XWmvZMmSZrqdmjx5sowePVoWLVokoaGhcurUKbM9T5485qFJqc6dO0tUVJR8+eWXpoeVcx9tkK7JMAAAgNtFYgoAACAL6Nq1q5w5c8YkmzSBVLt2bVm7dq2rIfrRo0fNSn1O2itKV/PT5JO7sLAwGTNmjBw/flxWr15ttumx3Gn11H333WfJdQEAgKyNxBQAAEAWodP2kpu6p43N3f3xxx8pHkurqLTZOQAAQHr6/9tmAAAAAAAAgIVITAEAAAAAAMAWJKYAAAAAAABgCxJTAAAANtAeTuPGjTNNyQEAAHwViSkAAAAbDBo0SFasWCHly5eXli1bypIlS+T69et2nxYAAIClSEwBAADYlJjatWuXREZGSpUqVeTFF1+UEiVKmFX1oqKi7D49AAAAS5CYAgAAsFGdOnXknXfekRMnTkhYWJjMnz9f6tevL7Vr15bw8HBxOBx2nyIAAEC6yZF+hwYAAMCNxMXFycqVK2XBggUSEREhd999t/Tp00eOHTsmr732mqxfv14WLVpk92kCAACkCxJTAAAANtDpepqMWrx4sfj5+UmPHj3krbfeksqVK7v2efTRR031FAAAQFZFYgoAAMAGmnDSpuezZ8+WDh06iL+/f5J9ypUrJ48//rgt5wcAAGAFElMAAAA2OHz4sJQtWzbFfXLnzm2qqgAAALIqmp8DAADYIDo6Wn744Yck23Xb9u3bbTknAAAAq5GYAgAAsMELL7wgf/75Z5Ltx48fN68BAAD4AhJTAAAANvjll1+kTp06Sbbfdddd5jUAAABfQGIKAADABjlz5pTTp08n2X7y5EnJkYM2oAAAwDeQmAIAALBBq1atZPjw4XLhwgXXtvPnz8trr71mVusDAADwBdyOAwAAsMHUqVPl3nvvNSvz6fQ9tWvXLilWrJj8+9//tvv0AAAALEFiCgAAwAYlS5aUn376SRYuXCi7d++WXLlySe/evaVbt27i7+9v9+kBAABYgsQUAACATXLnzi39+vWz+zQAAABsQ2IKAADARroC39GjRyU2NtZj+yOPPGLbOQEAAFiFxBQAAIANDh8+LI8++qjs2bNHsmXLJg6Hw2zX71V8fLzNZwgAAJBBV+X7888/5dixY67nkZGRMmjQIJk7d25anhsAAECWNXDgQClXrpxER0dLUFCQ/Pzzz7J582apV6+ebNq0ye7TAwAAyLiJqSeeeEI2btxovj916pRZ0liTUyNGjJBx48al9TkCAABkOVu3bjVxU3BwsPj5+ZlHkyZNZOLEiTJgwAC7Tw8AACDjJqb27t0rDRo0MN9/+umnUr16ddmyZYtZVebDDz9M63MEAADIcnSqXt68ec33mpw6ceKE+b5s2bKyf/9+m88OAAAgA/eYiouLk5w5c5rv169f72rOWblyZTl58mTaniEAAEAWpDf2du/ebabzNWzYUKZMmSIBAQGmNUL58uXtPj0AAICMWzFVrVo1mTNnjvznP/+RiIgIadOmjdmud/oKFy6c1ucIAACQ5YwcOVISEhLM9zql7/fff5emTZvKmjVr5J133rH79AAAADJuxdTkyZPNKjJvvvmm9OzZU2rVqmW2r1692jXFDwAAAMlr3bq16/sKFSrIvn375Ny5c1KwYEHXynwAAABZ3S0lpu677z45e/asXLx40QRPTv369TOrygAAACDltgi5cuWSXbt2mSl9ToUKFbL1vAAAADLFVL6rV6/K9evXXUmpI0eOyIwZM0yjzqJFi6b1OQIAAGQp/v7+UqZMGdMAHQAAwJfdUmKqffv28vHHH5vvz58/bxp2Tps2TTp06CCzZ89O63MEAADIckaMGCGvvfaamb4HAADgq24pMRUVFWWac6rPPvtMihUrZqqmNFlFs04AAIAbmzlzpmzevFlCQkKkUqVKUqdOHY8HAACAL7ilHlMxMTGSN29e8/26deukY8eO4ufnJ3fffbdJUAEAACBlWmkOAADg624pMaUrx6xatcqszPfNN9/I4MGDzfbo6GjJly9fWp8jAABAlhMWFmb3KQAAAGTOqXyjR4+Wl19+WUJDQ6VBgwbSqFEjV/XUXXfdldbnCAAAAAAAgCzoliqmOnfuLE2aNJGTJ09KrVq1XNubN29uqqgAAACQMm2DkC1btmRfZ8U+AADgC24pMaWKFy9uHseOHTPPS5UqZaqnAAAAcGMrV670eB4XFyc7d+6Ujz76SMaOHWvbeQEAAGT4xFRCQoK8/vrrMm3aNLl8+bLZps3QX3rpJbP0sd4BBAAAQPLat2/vtSq9WrVqsnTpUunTp48t5wUAAJDhE1OafPrggw9k0qRJcs8995ht33//vYwZM0auXbsmEyZMSOvzBAAA8Am6ynG/fv3sPg0AAICMm5jSEvP58+fLI4884tpWs2ZNKVmypDz//PMkpgAAAG7B1atX5Z133jExFQAAgC+4pcTUuXPnpHLlykm26zZ9DQAAACkrWLCgR/Nzh8Mhly5dkqCgIPnkk09sPTcAAIAMnZjSlfhmzpxp7ui5021aOQUAAICUvfXWWx6JKe3RWaRIEWnYsKFJWgEAAPiCW0pMTZkyRdq1ayfr16+XRo0amW1bt26VP//8U9asWZPW5wgAAJDl9OrVy+5TAAAAsN0tLZ/XrFkzOXDggDz66KNy/vx58+jYsaP8/PPP8u9//zvtzxIAACCLWbBggSxbtizJdt2m/TwBAAB8wS0lplRISIhpcr58+XLzeP311+Wvv/4yq/UBAAAgZRMnTpTg4OAk24sWLSpvvPGGLecEAACQaRJTAAAAuHVHjx6VcuXKJdletmxZ8xoAAIAvIDEFAABgA62M+umnn5Js3717txQuXNiWcwIAALAaiSkAAAAbdOvWTQYMGCAbN26U+Ph48/j2229l4MCB8vjjj9t9egAAABlvVT5tcJ4SbYIOAACAGxs/frz88ccf0rx5c8mR45+QLCEhQXr06EGPKQAA4DNuKjGVP3/+G76uwRQAAABSFhAQIEuXLjULyOzatUty5colNWrUMD2mAAAAfEWOm13WGAAAAGmnYsWK5gEAAOCL6DEFAABgg06dOsnkyZOTbJ8yZYp06dLFlnMCAACwGokpAAAAG2zevFkefPDBJNvbtm1rXgMAAPAFJKYAAABscPnyZdNnKjF/f3+5ePGiLecEAADgk4mpWbNmSWhoqAQGBkrDhg0lMjIyxf2XLVsmlStXNvtrk9A1a9Z4vH769Gnp1auXhISESFBQkLRp00YOHjzoev3cuXPy4osvSqVKlUyj0TJlypjlmi9cuJBu1wgAAOBOYxhtfp7YkiVLpGrVqracEwAAQIZufp4eNCAbMmSIzJkzxySlZsyYIa1bt5b9+/dL0aJFk+y/ZcsW6datm0ycOFEeeughWbRokXTo0EGioqKkevXq4nA4zHO92/j5559Lvnz5ZPr06dKiRQv55ZdfJHfu3HLixAnzmDp1qgn8jhw5Is8++6zZ9tlnn9kyDgAAwLeMGjVKOnbsKIcOHZIHHnjAbNuwYYOJbYhHAACAr7C9YkqTRn379pXevXubJJEmqLTKKTw83Ov+b7/9tqmAGjp0qFSpUkXGjx8vderUkZkzZ5rXtTJq27ZtMnv2bKlfv76pitLvr169KosXLzb7aAJr+fLl8vDDD8sdd9xhgsEJEybIF198IX///bel1w8AAHyTxiGrVq2S3377TZ5//nl56aWX5Pjx4/Ltt99KhQoV7D49AACArF8xFRsbKzt27JDhw4e7tvn5+Znqpq1bt3p9j27XCit3WmGlgZ26fv26+arT/NyPmTNnTvn+++/l6aef9npcncan1VU5cngfEj2u89jK2fshLi7OPPAP51gwJtZhzK3HmNuDcbceY+5dWo5Hu3btzMMZW+hNtJdfftnER/Hx8Wn2OQAAABmVrYmps2fPmqCrWLFiHtv1+b59+7y+59SpU1731+1Ke09pzyhNdr3//vtm6t5bb70lx44dk5MnTyZ7Hlp51a9fv2TPVacOjh07Nsn2devWmQoveIqIiLD7FHwOY249xtwejLv1GHNPMTExaXo8XYHvgw8+MNXc2h9Tp/dp/00AAABfYHuPqbSmvaVWrFghffr0kUKFCkn27NlNBZYuvaz9pxLTu5N6p1KnEY4ZMybZ42qiy71SS99XunRpadWqlam0wv/fRdZ/wLRs2dL8LJD+GHPrMeb2YNytx5h7lxYr5ukNtQ8//NAkpPR4jz32mKnM1gpwGp8DAABfYmtiKjg42CSOdBU9d/q8ePHiXt+j22+0f926dWXXrl1mep5OFyxSpIhprF6vXj2P9126dMn0q8qbN6+sXLkyxaBbpwLqIzF9D8F6UoyL9Rhz6zHm9mDcrceYe7rdsdDeUlolpTfGdNEXjUU0HtI+mwAAAL7G1ubnAQEBJomkK9A4JSQkmOeNGjXy+h7d7r6/0ru53vbPnz+/SUppQ/Tt27dL+/btXa/p3UmtdtJzWL16tUdPKgAAgPTy9ddfm8pubRGgySlNSgEAAPgq21fl0+lx8+bNk48++kh+/fVXee655+TKlStmlT7Vo0cPj+boAwcOlLVr18q0adNMHyqdfqdJp/79+7v2WbZsmWzatEkOHz4sn3/+uZmC0KFDB5OIck9K6ec4S+i1pF4fNBoFAADpSRdj0aptvTmnFd26srD2uwQAAPBFtveY6tq1q5w5c0ZGjx5tEkO1a9c2iSdng/OjR4+aVfWcGjduLIsWLZKRI0fKa6+9JhUrVjT9GKpXr+7aR5uca8JLp/iVKFHCJLdGjRrlej0qKkp++OEH833i5Zh///13CQ0NteDKAQCAL7r77rvNQ6fxLV26VMLDw03colXjWgWuPSy1zQAAAIAvsD0xpbTayb3iyZ1WPiXWpUsX80jOgAEDzCM59913n9dG6AAAAFbRlYOfeuop89i/f7+p4p40aZIMGzbMVHtrqwEAAICszvapfAAAAL6uUqVKMmXKFDl27JgsXrzY7tMBAACwDIkpAACADEIboWtfTKqlAACAryAxBQAAAAAAAFuQmAIAAMgiZs2aZRZxCQwMNCv+RUZGJruvrorctGlTKViwoHm0aNEiyf7ak1MXqNHFZHLlymX2OXjwoAVXAgAAfAWJKQAAgCxAV/jT1f3CwsLMCsS1atWS1q1bS3R0dLILzHTr1k02btwoW7duNasBtmrVSo4fP+7aR/tevfPOOzJnzhyzorE2bNdjXrt2zcIrAwAAWRmJKQAAgCxg+vTp0rdvX+ndu7dUrVrVJJOCgoIkPDzc6/4LFy6U559/XmrXri2VK1eW+fPnS0JCgmzYsMFVLTVjxgwZOXKktG/fXmrWrCkff/yxnDhxQlatWmXx1QEAgKwqh90nAAAAgNsTGxsrO3bskOHDh7u2+fn5mal3Wg2VGjExMRIXFyeFChUyz3///Xc5deqUOYZT/vz5zRRBPebjjz+e5BjXr183D6eLFy+ar3pcfeAfzrFgTKzDmNuDcbceY249xty7mxkPElMAAACZ3NmzZyU+Pl6KFSvmsV2f79u3L1XHePXVVyUkJMSViNKklPMYiY/pfC2xiRMnytixY5NsX7dunanegqeIiAi7T8HnMOb2YNytx5hbjzFPesMrtUhMAQAA+LhJkybJkiVLTN8pbZx+q7RiS/tcuVdMOXtX5cuXL43ONmvcRdZ/wLRs2VL8/f3tPh2fwJjbg3G3HmNuPcbcO2fVdGqQmAIAAMjkgoODJXv27HL69GmP7fq8ePHiKb536tSpJjG1fv1600fKyfk+PYauyud+TO1L5U3OnDnNIzEN1AnWk2JcrMeY24Nxtx5jbj3G3NPNjAXNzwEAADK5gIAAqVu3rqtxuXI2Mm/UqFGy79NV98aPHy9r166VevXqebxWrlw5k5xyP6be/dTV+VI6JgAAwM2gYgoAACAL0Cl0PXv2NAmmBg0amBX1rly5YlbpUz169JCSJUuaPlBq8uTJMnr0aFm0aJGEhoa6+kblyZPHPLJlyyaDBg2S119/XSpWrGgSVaNGjTJ9qDp06GDrtQIAgKyDxBQAAEAW0LVrVzlz5oxJNmmSSafbaSWUs3n50aNHzUp9TrNnzzar+XXu3NnjOGFhYTJmzBjz/SuvvGKSW/369ZPz589LkyZNzDFvpw8VAACAOxJTAAAAWUT//v3NwxttbO7ujz/+uOHxtGpq3Lhx5gEAAJAe6DEFAAAAAAAAW5CYAgAAAAAAgC1ITAEAAAAAAMAWJKYAAAAAAABgCxJTAAAAAAAAsAWJKQAAAAAAANiCxBQAAAAAAABsQWIKAAAAAAAAtiAxBQAAAAAAAFuQmAIAAAAAAIAtSEwBAAAAAADAFiSmAAAAAAAAYAsSUwAAAAAAALAFiSkAAAAAAADYgsQUAAAAAAAAbEFiCgAAAAAAALYgMQUAAAAAAABbkJgCAAAAAACALUhMAQAAAAAAwBYkpgAAAAAAAGALElMAAAAAAACwBYkpAAAAAAAA2ILEFAAAAAAAAGxBYgoAAAAAAAC2IDEFAAAAAAAAW5CYAgAAAAAAgC1ITAEAAAAAAMAWJKYAAAAAAABgCxJTAAAAAAAAsAWJKQAAAAAAANiCxBQAAAAAAABsQWIKAAAAAAAAtiAxBQAAAAAAAFuQmAIAAAAAAIAtSEwBAAAAAADAFiSmAAAAAAAAYAsSUwAAAAAAALAFiSkAAAAAAADYgsQUAAAAAAAAbEFiCgAAAAAAALYgMQUAAAAAAABbkJgCAAAAAACALUhMAQAAAAAAwBYkpgAAAAAAAGALElMAAAAAAACwBYkpAAAAAAAA2ILEFAAAAAAAAGxBYgoAAAAAAAC2IDEFAAAAAAAAW5CYAgAAAAAAgC1ITAEAAAAAAMAWJKYAAAAAAABgCxJTAAAAAAAAsAWJKQAAAAAAAPhuYmrWrFkSGhoqgYGB0rBhQ4mMjExx/2XLlknlypXN/jVq1JA1a9Z4vH769Gnp1auXhISESFBQkLRp00YOHjzosc+1a9fkhRdekMKFC0uePHmkU6dO5n0AAAAAAADwkcTU0qVLZciQIRIWFiZRUVFSq1Ytad26tURHR3vdf8uWLdKtWzfp06eP7Ny5Uzp06GAee/fuNa87HA7z/PDhw/L555+bfcqWLSstWrSQK1euuI4zePBg+eKLL0yS67vvvpMTJ05Ix44dLbtuAAAAAAAAX2d7Ymr69OnSt29f6d27t1StWlXmzJljqpzCw8O97v/222+bCqihQ4dKlSpVZPz48VKnTh2ZOXOmeV0ro7Zt2yazZ8+W+vXrS6VKlcz3V69elcWLF5t9Lly4IB988IH57AceeEDq1q0rCxYsMEkvfS8AAAAAAADSXw6xUWxsrOzYsUOGDx/u2ubn52eqm7Zu3er1PbpdK6zcaYXVqlWrzPfXr183X3Wan/sxc+bMKd9//708/fTT5jPj4uLM5zjp1MAyZcqY4999991JPleP6zy2unjxovmqx9EH/uEcC8bEOoy59RhzezDu1mPMvWM8AAAAskhi6uzZsxIfHy/FihXz2K7P9+3b5/U9p06d8rq/bndPMGmy6/3335fcuXPLW2+9JceOHZOTJ0+6jhEQECAFChRI9jiJTZw4UcaOHZtk+7p160yFFzxFRETYfQo+hzG3HmNuD8bdeoy5p5iYGLtPAQAAIMuwNTGVHvz9/WXFihWmB1WhQoUke/bspjKqbdu2pv/UrdJEl3ulllZMlS5dWlq1aiX58uVLo7PPGneR9R8wLVu2ND8LpD/G3HqMuT0Yd+sx5t45q6YBAACQyRNTwcHBJnGUeDU8fV68eHGv79HtN9pfe0bt2rXL9JLS6YJFihQxq/3Vq1fPdQzdfv78eY+qqZQ+V6cC6iMxDdQJ1pNiXKzHmFuPMbcH4249xtwTYwEAAJBFmp/rdDpNIm3YsMG1LSEhwTxv1KiR1/fodvf9ld7N9bZ//vz5TVJKG6Jv375d2rdvb7brZ2pQ6X6c/fv3y9GjR5P9XAAAAAAAAGSxqXw6Pa5nz56mmqlBgwYyY8YMuXLlilmlT/Xo0UNKlixpejypgQMHSrNmzWTatGnSrl07WbJkiUk6zZ0713XMZcuWmYSU9pras2ePeU+HDh3MtDtnwkqn+uln63Q/nYr34osvmqSUt8bnAAAAAAAAyIKJqa5du8qZM2dk9OjRpvF47dq1Ze3ata4G51rFpKvqOTVu3FgWLVokI0eOlNdee00qVqxoVuSrXr26ax9tcq5JJ52aV6JECZPcGjVqlMfnakN0PW6nTp3Manu6st97771n4ZUDAAAAAAD4NtsTU6p///7m4c2mTZuSbOvSpYt5JGfAgAHmkZLAwECZNWuWeQAAAAAAAMDHekwBAAAAAADAd5GYAgAAyAK0Cjw0NNRUhetqxJGRkcnu+/PPP5t2Brp/tmzZTI/PxOLj400rhHLlykmuXLnkjjvukPHjx4vD4UjnKwEAAL6ExBQAAEAmt3TpUtNfMywsTKKioqRWrVqmf2Z0dLTX/WNiYqR8+fIyadIkKV68uNd9Jk+eLLNnz5aZM2fKr7/+ap5PmTJF3n333XS+GgAA4EtITAEAAGRy06dPl759+5pVjatWrSpz5syRoKAgCQ8P97p//fr15c0335THH39ccubM6XWfLVu2SPv27c0qyFpZ1blzZ7PCcUqVWAAAAJmy+TkAAABuTWxsrOzYsUOGDx/u2qYrD7do0UK2bt16y8fVlZDnzp0rBw4ckDvvvFN2794t33//vUmCJUdXOtaH08WLF83XuLg488A/nGPBmFiHMbcH4249xtx6jLl3NzMeJKYAAAAysbNnz5p+UMWKFfPYrs/37dt3y8cdNmyYSSxVrlxZsmfPbj5jwoQJ0r1792TfM3HiRBk7dmyS7evWrTMVXPAUERFh9yn4HMbcHoy79Rhz6zHmSdsGpBaJKQAAACTx6aefysKFC2XRokVSrVo12bVrlwwaNEhCQkKkZ8+eXt+jVVva68pJE1ulS5c2UwDz5ctn4dln/LvI+g+Yli1bir+/v92n4xMYc3sw7tZjzK3HmHvnrJpODRJTAAAAmVhwcLCpaDp9+rTHdn2eXGPz1Bg6dKipmtI+VKpGjRpy5MgRUxWVXGJK+1V561mlgTrBelKMi/UYc3sw7tZjzK3HmHu6mbGg+TkAAEAmFhAQIHXr1pUNGza4tiUkJJjnjRo1uq0SfO1V5U4TYHpsAACAtELFFAAAQCan0+e0iqlevXrSoEEDmTFjhly5csWs0qd69OghJUuWNNVOzobpv/zyi+v748ePm6l6efLkkQoVKpjtDz/8sOkpVaZMGTOVb+fOnabx+VNPPWXjlQIAgKyGxBQAAEAm17VrVzlz5oyMHj1aTp06JbVr15a1a9e6GqIfPXrUo/rpxIkTctddd7meT5061TyaNWsmmzZtMtveffddGTVqlDz//PMSHR1teks988wz5jMAAADSCokpAACALKB///7m4Y0z2eQUGhoqDocjxePlzZvXVF7pAwAAIL3QYwoAAAAAAAC2IDEFAAAAAAAAW5CYAgAAAAAAgC1ITAEAAAAAAMAWJKYAAAAAAABgCxJTAAAAAAAAsAWJKQAAAAAAANiCxBQAAAAAAABsQWIKAAAAAAAAtiAxBQAAAAAAAFuQmAIAAAAAAIAtSEwBAAAAAADAFiSmAAAAAAAAYAsSUwAAAAAAALAFiSkAAAAAAADYgsQUAAAAAAAAbEFiCgAAAAAAALYgMQUAAAAAAABbkJgCAAAAAACALUhMAQAAAAAAwBYkpgAAAAAAAGALElMAAAAAAACwBYkpAAAAAAAA2ILEFAAAAAAAAGxBYgoAAAAAAAC2IDEFAAAAAAAAW5CYAgAAAAAAgC1ITAEAAAAAAMAWJKYAAAAAAABgCxJTAAAAAAAAsAWJKQAAAAAAANiCxBQAAAAAAABsQWIKAAAAAAAAtiAxBQAAAAAAAFuQmAIAAAAAAIAtSEwBAAAAAADAFiSmAAAAAAAAYAsSUwAAAAAAALAFiSkAAAAAAADYgsQUAAAAAAAAbEFiCgAAAAAAALYgMQUAAAAAAABbkJgCAAAAAACALUhMAQAAAAAAwBYkpgAAAAAAAGALElMAAAAAAACwBYkpAAAAAAAA2ILEFAAAAAAAAGxBYgoAAAAAAAC2IDEFAAAAAAAAW5CYAgAAAAAAgC1ITAEAAAAAAMAWJKYAAAAAAABgCxJTAAAAAAAAsAWJKQAAAAAAANiCxBQAAAAAAAB8MzE1a9YsCQ0NlcDAQGnYsKFERkamuP+yZcukcuXKZv8aNWrImjVrPF6/fPmy9O/fX0qVKiW5cuWSqlWrypw5czz2OXXqlDz55JNSvHhxyZ07t9SpU0eWL1+eLtcHAAAAAACADJiYWrp0qQwZMkTCwsIkKipKatWqJa1bt5bo6Giv+2/ZskW6desmffr0kZ07d0qHDh3MY+/eva599Hhr166VTz75RH799VcZNGiQSVStXr3atU+PHj1k//79ZtuePXukY8eO8thjj5ljAgAAAAAAwAcSU9OnT5e+fftK7969XZVNQUFBEh4e7nX/t99+W9q0aSNDhw6VKlWqyPjx402108yZMz2SVz179pT77rvPVGL169fPJLzcK7F0nxdffFEaNGgg5cuXl5EjR0qBAgVkx44dllw3AAAAAAAAbExMxcbGmkRQixYt/v9k/PzM861bt3p9j253319phZX7/o0bNzaVUMePHxeHwyEbN26UAwcOSKtWrTz20Wqtc+fOSUJCgixZskSuXbtmklkAAAAAAACwRg6xydmzZyU+Pl6KFSvmsV2f79u3z+t7tDeUt/11u9O7775rqqS0x1SOHDlMsmvevHly7733uvb59NNPpWvXrlK4cGGzj1ZprVy5UipUqJDs+V6/ft08nC5evGi+xsXFmQf+4RwLxsQ6jLn1GHN7MO7WY8y9YzwAAACyQGIqvWhiatu2baZqqmzZsrJ582Z54YUXJCQkxFVtNWrUKDl//rysX79egoODZdWqVabH1H/+8x/TUN2biRMnytixY5NsX7dunUlswVNERITdp+BzGHPrMeb2YNytx5h7iomJsfsUAAAAsgzbElOaEMqePbucPn3aY7s+19XyvNHtKe1/9epVee2110z1U7t27cy2mjVryq5du2Tq1KkmMXXo0CHTk0obplerVs3soz2oNCmlKwQmXsHPafjw4aaxunvFVOnSpc0UwXz58t3maGStu8j6D5iWLVuKv7+/3afjExhz6zHm9mDcrceYe+esmgYAAEAmTkwFBARI3bp1ZcOGDWZlPaX9nvS5rqLnTaNGjczrutKekwbMut19Wp1O33OnCTA9tvtdzpT28SZnzpzmkZgG6gTrSTEu1mPMrceY24Nxtx5j7omxAAAAyCJT+bQCSVfQq1evnlkhb8aMGXLlyhWzSp/q0aOHlCxZ0kyjUwMHDpRmzZrJtGnTTEWUNi3fvn27zJ0717yulUv6uq7alytXLjOV77vvvpOPP/7YrACoKleubHpJPfPMM6aKSvtM6VQ+TXB9+eWXNo4GAAAAAACAb7E1MaUNyM+cOSOjR482Dcxr164ta9eudTU4P3r0qEdlk66mt2jRIhk5cqSZslexYkWTVKpevbprH01W6bS77t27m1X3NDk1YcIEefbZZ113OdesWSPDhg2Thx9+WC5fvmwSVR999JE8+OCDNowCAAAAAACAb7K9+blO20tu6t6mTZuSbOvSpYt5JEf7TS1YsCDFz9SE1vLly2/hbAEAADIm7ZX55ptvmpt92j9TF4TRinRvfv75Z3NjcMeOHXLkyBF56623PFolOB0/flxeffVV+frrr007BL2Zp3GWVrsDAACkBc9GSwAAAMh0li5dalokhIWFSVRUlElMtW7dWqKjo73ur0mm8uXLy6RJk5JddOavv/6Se+65x1Sba2Lql19+Me0UChYsmM5XAwAAfIntFVMAAAC4PdpLs2/fvq4+nbrK8FdffSXh4eGmfUFi9evXNw/l7XU1efJkswKxeyV6uXLl0u0aAACAb6JiCgAAIBOLjY01U/JatGjh2qY9OvX51q1bb/m4q1evNlP2tIVC0aJF5a677pJ58+al0VkDAAD8g4opAACATOzs2bMSHx/vWjzGSZ/v27fvlo97+PBhmT17tpkiqIvO/PjjjzJgwAAJCAgwqyp7c/36dfNwunjxovkaFxdnHviHcywYE+sw5vZg3K3HmFuPMffuZsaDxBQAAACSSEhIMBVTb7zxhnmuFVN79+410wSTS0xNnDhRxo4dm2T7unXrJCgoKN3PObOJiIiw+xR8DmNuD8bdeoy59RjzpP0sU4vEFAAAQCYWHBws2bNnl9OnT3ts1+fJNTZPjRIlSkjVqlU9tlWpUiXFlY2HDx9uKqzcK6a0T1WrVq0kX758t3wuWfEusv4DpmXLlqa5PNIfY24Pxt16jLn1GHPvnFXTqUFiCgAAIBPTqXV169aVDRs2SIcOHVzVTvq8f//+t3xcXZFv//79HtsOHDggZcuWTfY9OXPmNI/ENFAnWE+KcbEeY24Pxt16jLn1GHNPNzMWJKYAAAAyOa1S0ul1OvWuQYMGMmPGDLly5Yprlb4ePXpIyZIlzVQ7Z8P0X375xfX98ePHZdeuXZInTx6pUKGC2T548GBp3Lixmcr32GOPSWRkpMydO9c8AAAA0gqJKQAAgEyua9eucubMGRk9erScOnVKateuLWvXrnU1RD969KhZqc/pxIkTpmeU09SpU82jWbNmsmnTJrOtfv36snLlSjM9b9y4cVKuXDmT8OrevbsNVwgAALIqElMAAABZgE7bS27qnjPZ5BQaGioOh+OGx3zooYfMAwAAIL38/60zAAAAAAAAwEIkpgAAAAAAAGALpvLdImf5+80sgegrS2XGxMSYcWFFAmsw5tZjzO3BuFuPMffO+bs/NVPhfB3xknf82bIeY24Pxt16jLn1GPPbj5dITN2iS5cuma+lS5e2+1QAAIBNsUD+/PntPo0MjXgJAADfdikV8VI2B7f7bklCQoJZ0SZv3rySLVs2u08nQ2VFNfj8888/JV++fHafjk9gzK3HmNuDcbceY+6dhk4aZIWEhHisdIekiJe848+W9RhzezDu1mPMrceY3368RMXULdKBLVWqlN2nkWHpH0j+UFqLMbceY24Pxt16jHlSVEqlDvFSyvizZT3G3B6Mu/UYc+sx5rceL3GbDwAAAAAAALYgMQUAAAAAAABbkJhCmsqZM6eEhYWZr7AGY249xtwejLv1GHMgffBny3qMuT0Yd+sx5tZjzG8fzc8BAAAAAABgCyqmAAAAAAAAYAsSUwAAAAAAALAFiSkAAAAAAADYgsQUUjRr1iwJDQ2VwMBAadiwoURGRia7b1xcnIwbN07uuOMOs3+tWrVk7dq1SfY7fvy4/Otf/5LChQtLrly5pEaNGrJ9+/Z0vhLfHvf4+HgZNWqUlCtXzoy57jt+/Hihxdw/Nm/eLA8//LCEhIRItmzZZNWqVTd8z6ZNm6ROnTqmyWGFChXkww8/vK2fo69JjzGfOHGi1K9fX/LmzStFixaVDh06yP79+9PxKjKX9Pr/3GnSpEnmuIMGDUrjMwcyPuIlexAvWYt4yXrES9YjXrKJNj8HvFmyZIkjICDAER4e7vj5558dffv2dRQoUMBx+vRpr/u/8sorjpCQEMdXX33lOHTokOO9995zBAYGOqKiolz7nDt3zlG2bFlHr169HD/88IPj8OHDjm+++cbx22+/WXhlvjfuEyZMcBQuXNjx5ZdfOn7//XfHsmXLHHny5HG8/fbbFl5ZxrVmzRrHiBEjHCtWrNDI07Fy5coU99f/b4OCghxDhgxx/PLLL453333XkT17dsfatWtv+efoa9JjzFu3bu1YsGCBY+/evY5du3Y5HnzwQUeZMmUcly9ftuCKfHPMnSIjIx2hoaGOmjVrOgYOHJiOVwFkPMRL9iBesh7xkvWIl6xHvGQPElNIVoMGDRwvvPCC63l8fLz5hT5x4kSv+5coUcIxc+ZMj20dO3Z0dO/e3fX81VdfdTRp0iQdzzrzS49xb9euneOpp55KcR/8IzW/gDS4rVatmse2rl27ml/0t/pz9GVpNeaJRUdHm2N/9913aXauWUVajvmlS5ccFStWdERERDiaNWtGoAWfQ7xkD+IlexEvWY94yXrES9ZhKh+8io2NlR07dkiLFi1c2/z8/MzzrVu3en3P9evXTQmuOy2D/v77713PV69eLfXq1ZMuXbqY0tG77rpL5s2bl45Xkrmk17g3btxYNmzYIAcOHDDPd+/ebV5v27Ztul1LVqY/C/efkWrdurXrZ3QrP0fc3ph7c+HCBfO1UKFC6X5+vjzmL7zwgrRr1y7JvoAvIF6yB/FS5kC8ZD3iJesRL6UNElPw6uzZs2aefbFixTy26/NTp055fY/+AZw+fbocPHhQEhISJCIiQlasWCEnT5507XP48GGZPXu2VKxYUb755ht57rnnZMCAAfLRRx+l+zX58rgPGzZMHn/8calcubL4+/ubAFfnNXfv3j3drykr0p+Ft5/RxYsX5erVq7f0c8TtjXli+mdB/x+/5557pHr16haeqW+N+ZIlSyQqKsr0qwB8EfGSPYiXMgfiJesRL1mPeCltkJhCmnn77bdNAKW/zAMCAqR///7Su3dvc+fD/S8/bQz3xhtvmF/2/fr1k759+8qcOXNsPfesPu6ffvqpLFy4UBYtWmT+UtTAdurUqQS4yLL0rtTevXtNIID08eeff8rAgQPN3y2JqxAAJI94yR7ES0BSxEvpj3gpdUhMwavg4GDJnj27nD592mO7Pi9evLjX9xQpUsSsWnDlyhU5cuSI7Nu3T/LkySPly5d37VOiRAmpWrWqx/uqVKkiR48eTacryVzSa9yHDh3quguoq/o8+eSTMnjwYLL2t0h/Ft5+Rvny5TPTAm7l54jbG3N3+o+NL7/8UjZu3CilSpWy+Ex9Z8x1+kV0dLT5x3OOHDnM47vvvpN33nnHfK93wYGsjnjJHsRLmQPxkvWIl6xHvJQ2SEzBK72TVLduXTPP3v3unT5v1KhRiu/VTHDJkiXl77//luXLl0v79u1dr2mZaOLlSHUef9myZdPhKjKf9Br3mJgYjzuCSgMBPTZunv4s3H9GSqcEOH9Gt/NzxK2NudIelRpkrVy5Ur799luz3DfSb8ybN28ue/bskV27drke2hNHp7zo9/p3DJDVES/Zg3gpcyBesh7xkvWIl9KIhY3Wkcno8q05c+Z0fPjhh2bpy379+pnlW0+dOmVef/LJJx3Dhg1z7b9t2zbH8uXLzRK8mzdvdjzwwAOOcuXKOf766y+PJTJz5MhhluM9ePCgY+HChWZ5zU8++cSWa/SVce/Zs6ejZMmSruWPdfnT4OBgs4oE/lklY+fOneahfy1Onz7dfH/kyBHzuo63jnviZWGHDh3q+PXXXx2zZs3yuvxxSj9HX5ceY/7cc8858ufP79i0aZPj5MmTrkdMTIwt1+gLY54Yq8zAFxEv2YN4yXrES9YjXrIe8ZI9SEwhRe+++66jTJkyjoCAALOcq/5Sd/8Dpb/AnfQvtypVqphfLoULFzZ/YI8fP57kmF988YWjevXqZr/KlSs75s6da9n1+Oq4X7x40fzlp8cMDAx0lC9f3jFixAjH9evXLb2ujGrjxo3mF0/ih3Oc9auOe+L31K5d2/yMdDwXLFhwUz9HX5ceY+7tePrw9rPxRen1/7k7Ai34KuIlexAvWYt4yXrES9YjXrJHNv1PWlVfAQAAAAAAAKlFjykAAAAAAADYgsQUAAAAAAAAbEFiCgAAAAAAALYgMQUAAAAAAABbkJgCAAAAAACALUhMAQAAAAAAwBYkpgAAAAAAAGALElMAAAAAAACwBYkpALBQtmzZZNWqVXafBgAAQIZFvAT4FhJTAHxGr169TKCT+NGmTRu7Tw0AACBDIF4CYLUcdp8AAFhJg6oFCxZ4bMuZM6dt5wMAAJDREC8BsBIVUwB8igZVxYsX93gULFjQvKZ3A2fPni1t27aVXLlySfny5eWzzz7zeP+ePXvkgQceMK8XLlxY+vXrJ5cvX/bYJzw8XKpVq2Y+q0SJEtK/f3+P18+ePSuPPvqoBAUFScWKFWX16tUWXDkAAEDqEC8BsBKJKQBwM2rUKOnUqZPs3r1bunfvLo8//rj8+uuv5rUrV65I69atTWD2448/yrJly2T9+vUegZQGai+88IIJwDQo0yCqQoUKHp8xduxYeeyxx+Snn36SBx980HzOuXPnLL9WAACAW0G8BCBNOQDAR/Ts2dORPXt2R+7cuT0eEyZMMK/rX4nPPvusx3saNmzoeO6558z3c+fOdRQsWNBx+fJl1+tfffWVw8/Pz3Hq1CnzPCQkxDFixIhkz0E/Y+TIka7neizd9vXXX6f59QIAANws4iUAVqPHFACfcv/995u7dO4KFSrk+r5Ro0Yer+nzXbt2me/1TmCtWrUkd+7crtfvueceSUhIkP3795vS9hMnTkjz5s1TPIeaNWu6vtdj5cuXT6Kjo2/72gAAANIC8RIAK5GYAuBTNLBJXCqeVrSPQmr4+/t7PNcATYM1AACAjIB4CYCV6DEFAG62bduW5HmVKlXM9/pVeylo7wSn//73v+Ln5yeVKlWSvHnzSmhoqGzYsMHy8wYAALAK8RKAtETFFACfcv36dTl16pTHthw5ckhwcLD5Xht01qtXT5o0aSILFy6UyMhI+eCDD8xr2nQzLCxMevbsKWPGjJEzZ87Iiy++KE8++aQUK1bM7KPbn332WSlatKhZrebSpUsmGNP9AAAAMgPiJQBWIjEFwKesXbvWLEnsTu/e7du3z7UCzJIlS+T55583+y1evFiqVq1qXtPlir/55hsZOHCg1K9f3zzXFWmmT5/uOpYGYdeuXZO33npLXn75ZRPAde7c2eKrBAAAuHXESwCslE07oFv6iQCQQWnvgpUrV0qHDh3sPhUAAIAMiXgJQFqjxxQAAAAAAABsQWIKAAAAAAAAtmAqHwAAAAAAAGxBxRQAAAAAAABsQWIKAAAAAAAAtiAxBQAAAAAAAFuQmAIAAAAAAIAtSEwBAAAAAADAFiSmAAAAAAAAYAsSUwAAAAAAALAFiSkAAAAAAADYgsQUAAAAAAAAxA7/Bxy7AAr6xbUpAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def resnet18_cifar(num_classes: int):\n",
    "    m = models.resnet18(weights=None)\n",
    "    # CIFAR10: 32x32，改第一层卷积 + 去掉 maxpool\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = resnet18_cifar(Data.num_classes).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0) 先从 loader 拿一个 batch 推断 input_dim\n",
    "\"\"\" xb, wb, yb = next(iter(train_loader))\n",
    "input_dim = int(np.prod(xb.shape[1:]))   # CIFAR10 通常是 3*32*32=3072\n",
    "\n",
    "print(\"xb shape:\", xb.shape, \"=> input_dim:\", input_dim)\n",
    "\n",
    "# 1) 模型：先 Flatten 再 MLP（这样即使 xb 是 4D 也能喂给 MLP）\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    MLP(\n",
    "        input_size=input_dim,\n",
    "        hidden_sizes=[],\n",
    "        output_size=Data.num_classes,\n",
    "        dropout_p=0,\n",
    "        bn=False,\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    " \"\"\"\n",
    "\n",
    "\"\"\" model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ") \"\"\"\n",
    "\n",
    "\"\"\" optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ") \"\"\"\n",
    "\n",
    "#optimizer = optim.SGD(\n",
    "#    model.parameters(),\n",
    "#    lr=0.01,\n",
    "#    momentum=0.0  # 0.9\n",
    "#)\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 1\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"PYTHONBREAKPOINT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89db1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 1. 固定随机种子\n",
    "torch.manual_seed(0)\n",
    "\n",
    "B, C = 5, 4\n",
    "\n",
    "logits = torch.randn(B, C, requires_grad=True)\n",
    "z = torch.randint(0, C, (B,))\n",
    "\n",
    "M = torch.rand(C, C)\n",
    "M = M / M.sum(dim=1, keepdim=True)\n",
    "F = M.clone()\n",
    "\n",
    "# 这里贴上你的 MarginalChainProperLoss 和 ForwardProperLoss 定义\n",
    "# loss_code=\"cross_entropy\"\n",
    "\n",
    "mc_loss_fn = MarginalChainProperLoss(M, loss_code=\"cross_entropy\", reduction=\"mean\")\n",
    "fw_loss_fn = ForwardProperLoss(F, loss_code=\"cross_entropy\", reduction=\"mean\")\n",
    "\n",
    "# Marginal chain\n",
    "logits_mc = logits.clone().detach().requires_grad_(True)\n",
    "loss_mc = mc_loss_fn(logits_mc, z)\n",
    "loss_mc.backward()\n",
    "grad_mc = logits_mc.grad.clone().detach()\n",
    "\n",
    "# Forward\n",
    "logits_fw = logits.clone().detach().requires_grad_(True)\n",
    "loss_fw = fw_loss_fn(logits_fw, z)\n",
    "loss_fw.backward()\n",
    "grad_fw = logits_fw.grad.clone().detach()\n",
    "\n",
    "print(\"loss_mc:\", loss_mc.item())\n",
    "print(\"loss_fw:\", loss_fw.item())\n",
    "print(\"loss diff:\", abs(loss_mc.item() - loss_fw.item()))\n",
    "\n",
    "print(\"grad same?\", torch.allclose(grad_mc, grad_fw, atol=1e-6))\n",
    "print(\"grad max diff:\", (grad_mc - grad_fw).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c50f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取一个 batch\n",
    "xb, zb, yb = next(iter(train_loader))   # 确保 zb 就是 z（weak index）\n",
    "xb = xb.to(device)\n",
    "zb = zb.to(device)\n",
    "\n",
    "logits = model(xb)\n",
    "\n",
    "fwd_loss_fn = ForwardProperLoss(weakener.M, \"cross_entropy\").to(device)\n",
    "ub_loss_fn  = UpperBoundWeakProperLoss(weakener.M, \"cross_entropy\").to(device)\n",
    "\n",
    "loss_fwd = fwd_loss_fn(logits, zb)\n",
    "loss_ub  = ub_loss_fn(logits, zb)\n",
    "\n",
    "print(\"loss_fwd:\", loss_fwd.item())\n",
    "print(\"loss_ub :\", loss_ub.item())\n",
    "\n",
    "g1 = torch.autograd.grad(loss_fwd, logits, retain_graph=True)[0]\n",
    "g2 = torch.autograd.grad(loss_ub,  logits)[0]\n",
    "print(\"grad norm fwd:\", g1.norm().item())\n",
    "print(\"grad norm ub :\",  g2.norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d20a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31519ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.015,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 90\n",
    "\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d1d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34351956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.015,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 400\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"tsallis_0.2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f39e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"tsallis_0.5\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3583bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"tsallis_0.2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3350bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Weak Train Loss')\n",
    "ax1.plot(clean_results['epoch'], clean_results['train_loss'], label='Supervised Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Weak Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Weak Test Accuracy')\n",
    "ax2.plot(clean_results['epoch'], clean_results['train_acc'],'--', label='Supervised Train Accuracy' )\n",
    "ax2.plot(clean_results['epoch'], clean_results['test_acc'], '--', label='Supervied Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfceed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
