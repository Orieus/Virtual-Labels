{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28621a1e",
   "metadata": {},
   "source": [
    "# Example of the usage of the Weak label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4dcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29cb012",
   "metadata": {},
   "source": [
    "We first need to load:\n",
    "\n",
    "1. **Standard Python libraries** for data handling and reproducibility.  \n",
    "2. **PyTorch** (and its submodules) for model definition, training, and data loading.  \n",
    "3. **Custom modules** from this project:\n",
    "   - **`train_test_loop`**: provides the `train_and_evaluate` function to run training and evaluation loops.  \n",
    "   - **`losses`**: contains various weak‐label‐aware loss functions like `FwdBwdLoss`.  \n",
    "   - **`weakener`**: implements the `Weakener` class for generating noisy/weak labels.  \n",
    "   - **`model`**: defines model architectures .\n",
    "   - **`dataset`**: provides `Data_handling` (and other dataset classes) for loading and splitting data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62fdf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# PyTorch core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom project modules\n",
    "from utils.train_test_loop import train_and_evaluate\n",
    "from utils.losses import FwdLoss, EMLoss, FwdBwdLoss, MarginalChainLoss\n",
    "from utils.losses1 import MarginalChainProperLoss, ForwardProperLoss, scoring_matrix\n",
    "from utils.losses1 import UpperBoundWeakProperLoss\n",
    "from utils.dataset_visualization import visualize_dataset\n",
    "from src.weakener import Weakener\n",
    "from src.model import MLP\n",
    "from src.dataset import Data_handling\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db405b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73f12c25",
   "metadata": {},
   "source": [
    "## Loading and Visualizing Iris\n",
    "\n",
    "1. **Instantiate** our `Data_handling` class to load the Iris dataset from OpenML (ID 61) using an 80/20 train/test split.  \n",
    "2. **Retrieve** the raw arrays of features and labels via `get_data()`.  \n",
    "3. **Combine** the train and test portions back into a single DataFrame \n",
    "4. **Visualize** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb6a1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Cifar10'\n",
    "Data = Data_handling(\n",
    "    # dataset='mnist',\n",
    "    dataset=dataset_name,\n",
    "    train_size=0.8,\n",
    "    test_size=0.2,\n",
    "    batch_size=64,\n",
    "    shuffling=False,\n",
    "    splitting_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ebde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data.train_dataset.data # This is Train_X\n",
    "Data.train_dataset.targets # This is Train_y\n",
    "print(Data.test_dataset.targets)\n",
    "df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "df['target'] = [i for i in Data.train_dataset.targets.numpy()]\n",
    "df \"\"\"\n",
    "#这段一直报错，但不影响实验结果，所以先block掉#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b001a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e126947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df_2_plot = df.iloc[0:1000]\n",
    "features = ['feature_102', 'feature_103']\n",
    "visualize_dataset(\n",
    "    df_2_plot,\n",
    "    features=features,\n",
    "    classes=Data.num_classes,\n",
    "    title=dataset_name,\n",
    ") \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df_2_plot[[features[0], features[1]]] \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27509ea8",
   "metadata": {},
   "source": [
    "Next, we’ll simulate a **partial‐label learning** or **noisy-label** setting by corrupting each true label with **M**:\n",
    "\n",
    "1. **Instantiate** a `Weakener` with the number of true classes.  \n",
    "2. **Build** a mixing matrix via `generate_M(model_class='pll', corr_p=…)` \n",
    "3. **Generate** weak labels with `generate_weak`, which returns:\n",
    "   - `z`: the integer index of the weak‐label   \n",
    "   - `w`: a binary matrix of shape `(n_samples, n_classes)` indicating the candidate labels  \n",
    "4. **Insert** the partial labels into our Data using `include_weak(w)`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_p = 0.2\n",
    "weakener = Weakener(true_classes=Data.num_classes)\n",
    "weakener.generate_M(model_class='pll', corr_p=0.2)\n",
    "# weakener.generate_M(model_class='unif_noise', corr_p=0.5) #Try this for noisy labels\n",
    "print(f\"Generated M matrix:\\n{weakener.M}\")\n",
    "true_onehot = Data.train_dataset.targets  # shape: (n_samples, n_classes)\n",
    "\n",
    "z = weakener.generate_weak(true_onehot)\n",
    "print(f\"Generated z (noisy labels):\\n{z}\")\n",
    "#print(f\"Generated w (multi-label matrix):\\n{w}\")\n",
    "\n",
    "Data.include_weak(z)\n",
    "\n",
    "train_loader, test_loader = Data.get_dataloader(weak_labels='weak')\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "xb, wb, yb = batch\n",
    "print(f\"Inputs batch shape: {xb.shape}\")\n",
    "print(f\"Weak (partial) labels shape: {wb.shape}\")\n",
    "print(f\"True one-hot labels shape: {yb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weak_df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "#df['target'] = [i for i in weakener.w.numpy()]\n",
    "#df\n",
    "\n",
    "# 1) 展平成 (N, 3072)\n",
    "X = Data.train_dataset.data                # (N, 3, 32, 32)  (torch tensor)\n",
    "X2 = X.view(X.shape[0], -1).cpu().numpy()  # (N, 3072)\n",
    "\n",
    "weak_df = pd.DataFrame(X2, columns=[f'feature_{i}' for i in range(X2.shape[1])])\n",
    "\n",
    "# 2) 加 true label（如果 targets 是 one-hot，就转成 class index）\n",
    "y = Data.train_dataset.targets\n",
    "if hasattr(y, \"ndim\") and y.ndim == 2:\n",
    "    y = y.argmax(dim=1)\n",
    "weak_df[\"target\"] = y.cpu().numpy()\n",
    "\n",
    "# 3) 加 weak label（weakener.w 可能是一维或二维：做个兼容）\n",
    "w = weakener.w\n",
    "w_np = w.detach().cpu().numpy()\n",
    "\n",
    "if w_np.ndim == 1:\n",
    "    weak_df[\"weak\"] = w_np\n",
    "else:\n",
    "    # 如果是 one-hot / multi-hot (N,C)，你可以：\n",
    "    # A) 每一类一列（适合做统计）\n",
    "    for c in range(w_np.shape[1]):\n",
    "        weak_df[f\"weak_{c}\"] = w_np[:, c]\n",
    "    # 或 B) 压缩成“候选集合”（适合阅读）\n",
    "    # weak_df[\"weak_set\"] = [np.flatnonzero(row).tolist() for row in w_np]\n",
    "\n",
    "weak_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_dataset(\n",
    "#     df,\n",
    "#     features=['feature_0', 'feature_1'],\n",
    "#     classes=3,\n",
    "#     title='Iris Samples with Pie Markers for Multi-Label Entries'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53b540",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Instantiate** the model (e.g. `MLP`) with its input/output dimensions.   \n",
    "2. **Choose** the optimizer and set hyperparameters.  \n",
    "3. **Define** the loss function.\n",
    "\n",
    "We also could do a learning rate scheduler (e.g. `StepLR`) to decrease the LR over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8ac2a",
   "metadata": {},
   "source": [
    "## Training the MLP (using `train_test_loop.py`)\n",
    "\n",
    "1. **Set** training hyperparameters  \n",
    "2. **Call** `train_and_evaluate(model, train_loader, test_loader, optimizer, pll_loss, num_epochs, corr_p)`\n",
    "3. **Plot** results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def resnet18_cifar(num_classes: int):\n",
    "    m = models.resnet18(weights=None)\n",
    "    # CIFAR10: 32x32，改第一层卷积 + 去掉 maxpool\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = resnet18_cifar(Data.num_classes).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 0) 先从 loader 拿一个 batch 推断 input_dim\n",
    "\"\"\" xb, wb, yb = next(iter(train_loader))\n",
    "input_dim = int(np.prod(xb.shape[1:]))   # CIFAR10 通常是 3*32*32=3072\n",
    "\n",
    "print(\"xb shape:\", xb.shape, \"=> input_dim:\", input_dim)\n",
    "\n",
    "# 1) 模型：先 Flatten 再 MLP（这样即使 xb 是 4D 也能喂给 MLP）\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    MLP(\n",
    "        input_size=input_dim,\n",
    "        hidden_sizes=[],\n",
    "        output_size=Data.num_classes,\n",
    "        dropout_p=0,\n",
    "        bn=False,\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    " \"\"\"\n",
    "\n",
    "\"\"\" model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ") \"\"\"\n",
    "\n",
    "\"\"\" optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ") \"\"\"\n",
    "\n",
    "#optimizer = optim.SGD(\n",
    "#    model.parameters(),\n",
    "#    lr=0.01,\n",
    "#    momentum=0.0  # 0.9\n",
    "#)\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 10\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"PYTHONBREAKPOINT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89db1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 1. 固定随机种子\n",
    "torch.manual_seed(0)\n",
    "\n",
    "B, C = 5, 4\n",
    "\n",
    "logits = torch.randn(B, C, requires_grad=True)\n",
    "z = torch.randint(0, C, (B,))\n",
    "\n",
    "M = torch.rand(C, C)\n",
    "M = M / M.sum(dim=1, keepdim=True)\n",
    "F = M.clone()\n",
    "\n",
    "# 这里贴上你的 MarginalChainProperLoss 和 ForwardProperLoss 定义\n",
    "# loss_code=\"cross_entropy\"\n",
    "\n",
    "mc_loss_fn = MarginalChainProperLoss(M, loss_code=\"cross_entropy\", reduction=\"mean\")\n",
    "fw_loss_fn = ForwardProperLoss(F, loss_code=\"cross_entropy\", reduction=\"mean\")\n",
    "\n",
    "# Marginal chain\n",
    "logits_mc = logits.clone().detach().requires_grad_(True)\n",
    "loss_mc = mc_loss_fn(logits_mc, z)\n",
    "loss_mc.backward()\n",
    "grad_mc = logits_mc.grad.clone().detach()\n",
    "\n",
    "# Forward\n",
    "logits_fw = logits.clone().detach().requires_grad_(True)\n",
    "loss_fw = fw_loss_fn(logits_fw, z)\n",
    "loss_fw.backward()\n",
    "grad_fw = logits_fw.grad.clone().detach()\n",
    "\n",
    "print(\"loss_mc:\", loss_mc.item())\n",
    "print(\"loss_fw:\", loss_fw.item())\n",
    "print(\"loss diff:\", abs(loss_mc.item() - loss_fw.item()))\n",
    "\n",
    "print(\"grad same?\", torch.allclose(grad_mc, grad_fw, atol=1e-6))\n",
    "print(\"grad max diff:\", (grad_mc - grad_fw).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c50f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取一个 batch\n",
    "xb, zb, yb = next(iter(train_loader))   # 确保 zb 就是 z（weak index）\n",
    "xb = xb.to(device)\n",
    "zb = zb.to(device)\n",
    "\n",
    "logits = model(xb)\n",
    "\n",
    "fwd_loss_fn = ForwardProperLoss(weakener.M, \"cross_entropy\").to(device)\n",
    "ub_loss_fn  = UpperBoundWeakProperLoss(weakener.M, \"cross_entropy\").to(device)\n",
    "\n",
    "loss_fwd = fwd_loss_fn(logits, zb)\n",
    "loss_ub  = ub_loss_fn(logits, zb)\n",
    "\n",
    "print(\"loss_fwd:\", loss_fwd.item())\n",
    "print(\"loss_ub :\", loss_ub.item())\n",
    "\n",
    "g1 = torch.autograd.grad(loss_fwd, logits, retain_graph=True)[0]\n",
    "g2 = torch.autograd.grad(loss_ub,  logits)[0]\n",
    "print(\"grad norm fwd:\", g1.norm().item())\n",
    "print(\"grad norm ub :\",  g2.norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d20a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31519ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.015,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 90\n",
    "\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d1d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34351956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.015,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 400\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"tsallis_0.2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f39e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"tsallis_0.5\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3583bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"tsallis_0.2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3350bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Weak Train Loss')\n",
    "ax1.plot(clean_results['epoch'], clean_results['train_loss'], label='Supervised Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Weak Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Weak Test Accuracy')\n",
    "ax2.plot(clean_results['epoch'], clean_results['train_acc'],'--', label='Supervised Train Accuracy' )\n",
    "ax2.plot(clean_results['epoch'], clean_results['test_acc'], '--', label='Supervied Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfceed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
