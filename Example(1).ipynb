{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28621a1e",
   "metadata": {},
   "source": [
    "# Example of the usage of the Weak label classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29cb012",
   "metadata": {},
   "source": [
    "We first need to load:\n",
    "\n",
    "1. **Standard Python libraries** for data handling and reproducibility.  \n",
    "2. **PyTorch** (and its submodules) for model definition, training, and data loading.  \n",
    "3. **Custom modules** from this project:\n",
    "   - **`train_test_loop`**: provides the `train_and_evaluate` function to run training and evaluation loops.  \n",
    "   - **`losses`**: contains various weak‐label‐aware loss functions like `FwdBwdLoss`.  \n",
    "   - **`weakener`**: implements the `Weakener` class for generating noisy/weak labels.  \n",
    "   - **`model`**: defines model architectures .\n",
    "   - **`dataset`**: provides `Data_handling` (and other dataset classes) for loading and splitting data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62fdf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# PyTorch core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom project modules\n",
    "from utils.train_test_loop import train_and_evaluate\n",
    "from utils.losses import FwdLoss, EMLoss, FwdBwdLoss, MarginalChainLoss\n",
    "from utils.losses1 import MarginalChainProperLoss, ForwardProperLoss, scoring_matrix\n",
    "from utils.losses1 import UpperBoundWeakProperLoss\n",
    "from utils.dataset_visualization import visualize_dataset\n",
    "from src.weakener import Weakener\n",
    "from src.model import MLP\n",
    "from src.dataset import Data_handling\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f12c25",
   "metadata": {},
   "source": [
    "## Loading and Visualizing Iris\n",
    "\n",
    "1. **Instantiate** our `Data_handling` class to load the Iris dataset from OpenML (ID 61) using an 80/20 train/test split.  \n",
    "2. **Retrieve** the raw arrays of features and labels via `get_data()`.  \n",
    "3. **Combine** the train and test portions back into a single DataFrame \n",
    "4. **Visualize** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e44cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'mnist'\n",
    "Data = Data_handling(\n",
    "    # dataset='mnist',\n",
    "    dataset=dataset_name,\n",
    "    train_size=0.8,\n",
    "    test_size=0.2,\n",
    "    batch_size=64,\n",
    "    shuffling=False,\n",
    "    splitting_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655ebde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_775</th>\n",
       "      <th>feature_776</th>\n",
       "      <th>feature_777</th>\n",
       "      <th>feature_778</th>\n",
       "      <th>feature_779</th>\n",
       "      <th>feature_780</th>\n",
       "      <th>feature_781</th>\n",
       "      <th>feature_782</th>\n",
       "      <th>feature_783</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "2            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "59995        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59996        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59997        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59998        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59999        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "       feature_6  feature_7  feature_8  feature_9  ...  feature_775  \\\n",
       "0            0.0        0.0        0.0        0.0  ...          0.0   \n",
       "1            0.0        0.0        0.0        0.0  ...          0.0   \n",
       "2            0.0        0.0        0.0        0.0  ...          0.0   \n",
       "3            0.0        0.0        0.0        0.0  ...          0.0   \n",
       "4            0.0        0.0        0.0        0.0  ...          0.0   \n",
       "...          ...        ...        ...        ...  ...          ...   \n",
       "59995        0.0        0.0        0.0        0.0  ...          0.0   \n",
       "59996        0.0        0.0        0.0        0.0  ...          0.0   \n",
       "59997        0.0        0.0        0.0        0.0  ...          0.0   \n",
       "59998        0.0        0.0        0.0        0.0  ...          0.0   \n",
       "59999        0.0        0.0        0.0        0.0  ...          0.0   \n",
       "\n",
       "       feature_776  feature_777  feature_778  feature_779  feature_780  \\\n",
       "0              0.0          0.0          0.0          0.0          0.0   \n",
       "1              0.0          0.0          0.0          0.0          0.0   \n",
       "2              0.0          0.0          0.0          0.0          0.0   \n",
       "3              0.0          0.0          0.0          0.0          0.0   \n",
       "4              0.0          0.0          0.0          0.0          0.0   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "59995          0.0          0.0          0.0          0.0          0.0   \n",
       "59996          0.0          0.0          0.0          0.0          0.0   \n",
       "59997          0.0          0.0          0.0          0.0          0.0   \n",
       "59998          0.0          0.0          0.0          0.0          0.0   \n",
       "59999          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "       feature_781  feature_782  feature_783  \\\n",
       "0              0.0          0.0          0.0   \n",
       "1              0.0          0.0          0.0   \n",
       "2              0.0          0.0          0.0   \n",
       "3              0.0          0.0          0.0   \n",
       "4              0.0          0.0          0.0   \n",
       "...            ...          ...          ...   \n",
       "59995          0.0          0.0          0.0   \n",
       "59996          0.0          0.0          0.0   \n",
       "59997          0.0          0.0          0.0   \n",
       "59998          0.0          0.0          0.0   \n",
       "59999          0.0          0.0          0.0   \n",
       "\n",
       "                                                  target  \n",
       "0      [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "1      [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2      [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3      [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                  ...  \n",
       "59995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
       "59996  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "59997  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "59998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
       "59999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.train_dataset.data # This is Train_X\n",
    "Data.train_dataset.targets # This is Train_y\n",
    "print(Data.test_dataset.targets)\n",
    "df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "df['target'] = [i for i in Data.train_dataset.targets.numpy()]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b001a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e126947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_2_plot = df.iloc[0:1000]\n",
    "features = ['feature_102', 'feature_103']\n",
    "visualize_dataset(\n",
    "    df_2_plot,\n",
    "    features=features,\n",
    "    classes=Data.num_classes,\n",
    "    title=dataset_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ce7928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_102</th>\n",
       "      <th>feature_103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_102  feature_103\n",
       "0            0.0          0.0\n",
       "1            0.0          0.0\n",
       "2            0.0          0.0\n",
       "3            0.0          0.0\n",
       "4            0.0          0.0\n",
       "..           ...          ...\n",
       "995          0.0          0.0\n",
       "996         85.0          0.0\n",
       "997          0.0          0.0\n",
       "998          0.0          0.0\n",
       "999          0.0        120.0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2_plot[[features[0], features[1]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27509ea8",
   "metadata": {},
   "source": [
    "Next, we’ll simulate a **partial‐label learning** or **noisy-label** setting by corrupting each true label with **M**:\n",
    "\n",
    "1. **Instantiate** a `Weakener` with the number of true classes.  \n",
    "2. **Build** a mixing matrix via `generate_M(model_class='pll', corr_p=…)` \n",
    "3. **Generate** weak labels with `generate_weak`, which returns:\n",
    "   - `z`: the integer index of the weak‐label   \n",
    "   - `w`: a binary matrix of shape `(n_samples, n_classes)` indicating the candidate labels  \n",
    "4. **Insert** the partial labels into our Data using `include_weak(w)`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2587776a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated M matrix:\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.34217728e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  1.34217728e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  3.35544320e-02 3.35544320e-02]\n",
      " ...\n",
      " [2.04800000e-06 2.04800000e-06 2.04800000e-06 ... 2.04800000e-06\n",
      "  0.00000000e+00 2.04800000e-06]\n",
      " [2.04800000e-06 2.04800000e-06 2.04800000e-06 ... 2.04800000e-06\n",
      "  2.04800000e-06 0.00000000e+00]\n",
      " [5.12000000e-07 5.12000000e-07 5.12000000e-07 ... 5.12000000e-07\n",
      "  5.12000000e-07 5.12000000e-07]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated M matrix:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mweakener.M\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m true_onehot = Data.train_dataset.targets  \u001b[38;5;66;03m# shape: (n_samples, n_classes)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m z, w = weakener.generate_weak(true_onehot)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated z (noisy labels):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mz\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated w (multi-label matrix):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "corr_p = 0.2\n",
    "weakener = Weakener(true_classes=Data.num_classes)\n",
    "weakener.generate_M(model_class='pll', corr_p=0.2)\n",
    "# weakener.generate_M(model_class='unif_noise', corr_p=0.5) #Try this for noisy labels\n",
    "print(f\"Generated M matrix:\\n{weakener.M}\")\n",
    "true_onehot = Data.train_dataset.targets  # shape: (n_samples, n_classes)\n",
    "\n",
    "z, w = weakener.generate_weak(true_onehot)\n",
    "print(f\"Generated z (noisy labels):\\n{z}\")\n",
    "print(f\"Generated w (multi-label matrix):\\n{w}\")\n",
    "\n",
    "Data.include_weak(z)\n",
    "\n",
    "train_loader, test_loader = Data.get_dataloader(weak_labels='weak')\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "xb, wb, yb = batch\n",
    "print(f\"Inputs batch shape: {xb.shape}\")\n",
    "print(f\"Weak (partial) labels shape: {wb.shape}\")\n",
    "print(f\"True one-hot labels shape: {yb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a69c4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_775</th>\n",
       "      <th>feature_776</th>\n",
       "      <th>feature_777</th>\n",
       "      <th>feature_778</th>\n",
       "      <th>feature_779</th>\n",
       "      <th>feature_780</th>\n",
       "      <th>feature_781</th>\n",
       "      <th>feature_782</th>\n",
       "      <th>feature_783</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "2            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "59995        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59996        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59997        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59998        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "59999        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "       feature_6  feature_7  feature_8  feature_9  ...  feature_775  \\\n",
       "0            0.0        0.0        0.0        0.0  ...          0.0   \n",
       "1            0.0        0.0        0.0        0.0  ...          0.0   \n",
       "2            0.0        0.0        0.0        0.0  ...          0.0   \n",
       "3            0.0        0.0        0.0        0.0  ...          0.0   \n",
       "4            0.0        0.0        0.0        0.0  ...          0.0   \n",
       "...          ...        ...        ...        ...  ...          ...   \n",
       "59995        0.0        0.0        0.0        0.0  ...          0.0   \n",
       "59996        0.0        0.0        0.0        0.0  ...          0.0   \n",
       "59997        0.0        0.0        0.0        0.0  ...          0.0   \n",
       "59998        0.0        0.0        0.0        0.0  ...          0.0   \n",
       "59999        0.0        0.0        0.0        0.0  ...          0.0   \n",
       "\n",
       "       feature_776  feature_777  feature_778  feature_779  feature_780  \\\n",
       "0              0.0          0.0          0.0          0.0          0.0   \n",
       "1              0.0          0.0          0.0          0.0          0.0   \n",
       "2              0.0          0.0          0.0          0.0          0.0   \n",
       "3              0.0          0.0          0.0          0.0          0.0   \n",
       "4              0.0          0.0          0.0          0.0          0.0   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "59995          0.0          0.0          0.0          0.0          0.0   \n",
       "59996          0.0          0.0          0.0          0.0          0.0   \n",
       "59997          0.0          0.0          0.0          0.0          0.0   \n",
       "59998          0.0          0.0          0.0          0.0          0.0   \n",
       "59999          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "       feature_781  feature_782  feature_783  \\\n",
       "0              0.0          0.0          0.0   \n",
       "1              0.0          0.0          0.0   \n",
       "2              0.0          0.0          0.0   \n",
       "3              0.0          0.0          0.0   \n",
       "4              0.0          0.0          0.0   \n",
       "...            ...          ...          ...   \n",
       "59995          0.0          0.0          0.0   \n",
       "59996          0.0          0.0          0.0   \n",
       "59997          0.0          0.0          0.0   \n",
       "59998          0.0          0.0          0.0   \n",
       "59999          0.0          0.0          0.0   \n",
       "\n",
       "                                                  target  \n",
       "0      [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "1      [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
       "2      [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "3      [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4      [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                  ...  \n",
       "59995  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
       "59996  [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
       "59997  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
       "59998  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...  \n",
       "59999  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_df = pd.DataFrame(Data.train_dataset.data.numpy(), columns=[f'feature_{i}' for i in range(Data.train_dataset.data.shape[1])])\n",
    "df['target'] = [i for i in weakener.w.numpy()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_dataset(\n",
    "#     df,\n",
    "#     features=['feature_0', 'feature_1'],\n",
    "#     classes=3,\n",
    "#     title='Iris Samples with Pie Markers for Multi-Label Entries'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53b540",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Instantiate** the model (e.g. `MLP`) with its input/output dimensions.   \n",
    "2. **Choose** the optimizer and set hyperparameters.  \n",
    "3. **Define** the loss function.\n",
    "\n",
    "We also could do a learning rate scheduler (e.g. `StepLR`) to decrease the LR over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8ac2a",
   "metadata": {},
   "source": [
    "## Training the MLP (using `train_test_loop.py`)\n",
    "\n",
    "1. **Set** training hyperparameters  \n",
    "2. **Call** `train_and_evaluate(model, train_loader, test_loader, optimizer, pll_loss, num_epochs, corr_p)`\n",
    "3. **Plot** results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "#optimizer = optim.SGD(\n",
    "#    model.parameters(),\n",
    "#    lr=0.01,\n",
    "#    momentum=0.0  # 0.9\n",
    "#)\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"PYTHONBREAKPOINT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89db1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 1. 固定随机种子\n",
    "torch.manual_seed(0)\n",
    "\n",
    "B, C = 5, 4\n",
    "\n",
    "logits = torch.randn(B, C, requires_grad=True)\n",
    "z = torch.randint(0, C, (B,))\n",
    "\n",
    "M = torch.rand(C, C)\n",
    "M = M / M.sum(dim=1, keepdim=True)\n",
    "F = M.clone()\n",
    "\n",
    "# 这里贴上你的 MarginalChainProperLoss 和 ForwardProperLoss 定义\n",
    "# loss_code=\"cross_entropy\"\n",
    "\n",
    "mc_loss_fn = MarginalChainProperLoss(M, loss_code=\"cross_entropy\", reduction=\"mean\")\n",
    "fw_loss_fn = ForwardProperLoss(F, loss_code=\"cross_entropy\", reduction=\"mean\")\n",
    "\n",
    "# Marginal chain\n",
    "logits_mc = logits.clone().detach().requires_grad_(True)\n",
    "loss_mc = mc_loss_fn(logits_mc, z)\n",
    "loss_mc.backward()\n",
    "grad_mc = logits_mc.grad.clone().detach()\n",
    "\n",
    "# Forward\n",
    "logits_fw = logits.clone().detach().requires_grad_(True)\n",
    "loss_fw = fw_loss_fn(logits_fw, z)\n",
    "loss_fw.backward()\n",
    "grad_fw = logits_fw.grad.clone().detach()\n",
    "\n",
    "print(\"loss_mc:\", loss_mc.item())\n",
    "print(\"loss_fw:\", loss_fw.item())\n",
    "print(\"loss diff:\", abs(loss_mc.item() - loss_fw.item()))\n",
    "\n",
    "print(\"grad same?\", torch.allclose(grad_mc, grad_fw, atol=1e-6))\n",
    "print(\"grad max diff:\", (grad_mc - grad_fw).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c50f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取一个 batch\n",
    "xb, zb, yb = next(iter(train_loader))   # 确保 zb 就是 z（weak index）\n",
    "xb = xb.to(device)\n",
    "zb = zb.to(device)\n",
    "\n",
    "logits = model(xb)\n",
    "\n",
    "fwd_loss_fn = ForwardProperLoss(weakener.M, \"cross_entropy\").to(device)\n",
    "ub_loss_fn  = UpperBoundWeakProperLoss(weakener.M, \"cross_entropy\").to(device)\n",
    "\n",
    "loss_fwd = fwd_loss_fn(logits, zb)\n",
    "loss_ub  = ub_loss_fn(logits, zb)\n",
    "\n",
    "print(\"loss_fwd:\", loss_fwd.item())\n",
    "print(\"loss_ub :\", loss_ub.item())\n",
    "\n",
    "g1 = torch.autograd.grad(loss_fwd, logits, retain_graph=True)[0]\n",
    "g2 = torch.autograd.grad(loss_ub,  logits)[0]\n",
    "print(\"grad norm fwd:\", g1.norm().item())\n",
    "print(\"grad norm ub :\",  g2.norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0822459b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializing MLP model...\n",
      "784\n",
      "[]\n",
      "10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     23\u001b[39m em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\u001b[33m\"\u001b[39m\u001b[33mcross_entropy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 3. Run the training + evaluation loop\u001b[39;00m\n\u001b[32m     27\u001b[39m model, results_df = train_and_evaluate(\n\u001b[32m     28\u001b[39m     model,        \u001b[38;5;66;03m# our MLP on device\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mtrain_loader\u001b[49m, \u001b[38;5;66;03m# yields (x, w, y)\u001b[39;00m\n\u001b[32m     30\u001b[39m     test_loader,  \u001b[38;5;66;03m# yields (x, y)\u001b[39;00m\n\u001b[32m     31\u001b[39m     optimizer,    \u001b[38;5;66;03m# Adam optimizer\u001b[39;00m\n\u001b[32m     32\u001b[39m     em_loss,     \u001b[38;5;66;03m# EMLoss with our PLL mixing matrix\u001b[39;00m\n\u001b[32m     33\u001b[39m     num_epochs,   \u001b[38;5;66;03m# total epochs\u001b[39;00m\n\u001b[32m     34\u001b[39m     corr_p        \u001b[38;5;66;03m# used for logging consistency\u001b[39;00m\n\u001b[32m     35\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 4. View the epoch‐by‐epoch results\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(results_df)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = UpperBoundWeakProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90568ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "#em_loss = FwdLoss(weakener.M)\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "#em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"cross_entropy\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d20a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31519ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-7,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 90\n",
    "\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"ps_2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34351956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = MarginalChainProperLoss(weakener.M, loss_code=\"tsallis_0.2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f39e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    input_size=Data.num_features,\n",
    "    hidden_sizes=[],\n",
    "    output_size=Data.num_classes,\n",
    "    dropout_p=0,\n",
    "    bn=False,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-6,\n",
    ")\n",
    "# 2. Training parameters\n",
    "num_epochs = 60\n",
    "\n",
    "em_loss = ForwardProperLoss(weakener.M, loss_code=\"tsallis_0.2\")\n",
    "\n",
    "# 3. Run the training + evaluation loop\n",
    "model, results_df = train_and_evaluate(\n",
    "    model,        # our MLP on device\n",
    "    train_loader, # yields (x, w, y)\n",
    "    test_loader,  # yields (x, y)\n",
    "    optimizer,    # Adam optimizer\n",
    "    em_loss,     # EMLoss with our PLL mixing matrix\n",
    "    num_epochs,   # total epochs\n",
    "    corr_p        # used for logging consistency\n",
    ")\n",
    "\n",
    "# 4. View the epoch‐by‐epoch results\n",
    "print(results_df)\n",
    "\n",
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3350bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a wide figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(results_df['epoch'], results_df['train_loss'], label='Weak Train Loss')\n",
    "ax1.plot(clean_results['epoch'], clean_results['train_loss'], label='Supervised Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(results_df['epoch'], results_df['train_acc'], label='Weak Train Accuracy')\n",
    "ax2.plot(results_df['epoch'], results_df['test_acc'], label='Weak Test Accuracy')\n",
    "ax2.plot(clean_results['epoch'], clean_results['train_acc'],'--', label='Supervised Train Accuracy' )\n",
    "ax2.plot(clean_results['epoch'], clean_results['test_acc'], '--', label='Supervied Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfceed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
